{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Jupyter-Notebook-Tutorial:   \n",
    "#    Anpassung von Modellen an Daten mit *kafe2* \n",
    "\n",
    "                                                Johannes Gäßler, Oktober 2022\n",
    "                                                Günter Quast, April 2020\n",
    "---\n",
    "## Grundsätzliches zu Jupyter Notebooks\n",
    "\n",
    "Diese Datei vom Typ `.ipynb` enthält ein Tutorial als `Jupyter notebook`.\n",
    "*Jupyter* bietet eine Browser-Schnittstelle mit einer (einfachen) Entwicklungsumgebung für\n",
    "*Python*-Code und erklärende Texte im intuitiven *Markdown*-Format.\n",
    "Die Eingabe von Formeln im *LaTeX*-Format wird ebenfalls unterstützt.\n",
    "\n",
    "Eine Zusammenstellung der wichtigsten Befehle zur Verwendung von *Jupyter* als Arbeitsumgebung\n",
    "findet sich im Notebook\n",
    "[*JupyterCheatsheet.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/JupyterCheatsheet.ipynb).\n",
    "Grundlagen zur statistischen Datenauswertung finden sich in den Notebooks \n",
    "[*IntroStatistik.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/IntroStatistik.ipynb)\n",
    "und\n",
    "[*Fehlerrechnung.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/Fehlerrechnung.ipynb).\n",
    "\n",
    "In *Jupyter* werden Code und Text in jeweils einzelne Zellen eingegeben. \n",
    "Aktive Zellen werden durch einen blauen Balken am Rand angezeigt.\n",
    "Sie können sich in zwei Zuständen befinden: im Edit-Mode ist das Eingabefeld weiß, im\n",
    "Command-Mode ist es ausgegraut.\n",
    "Durch Klicken in den Randbereich wird der Command-Mode gewählt, ein Klick in das Textfeld einer\n",
    "Code-Zelle schaltet in den Edit-Mode.\n",
    "Die Taste `esc` kann ebenfalls verwendet werden, um den Edit-Mode zu verlassen.\n",
    "\n",
    "Die Eingabe von `a` im Command-Mode erzeugt eine neue leere Zelle oberhalb der aktiven Zelle, `b`\n",
    "eine unterhalb. Eingabe von `dd` löscht die betreffende Zelle.\n",
    "\n",
    "Zellen können entweder den Typ `Markdown` oder `Code` haben.\n",
    "Die Eingabe von `m` im Command-Mode setzt den Typ Markdown, Eingabe von `y` wählt den Typ Code.\n",
    "\n",
    "Prozessiert - also Text gesetzt oder Code ausgeführt - wird der Zelleninhalt durch Eingabe von\n",
    "`shift+return`, oder auch `alt+return` wenn zusätzlich eine neue, leere Zelle erzeugt werden soll.\n",
    "\n",
    "Die hier genannten Einstellungen sowie das Einfügen, Löschen oder Ausführen von Zellen sind\n",
    "auch über das PullDown-Menü am oberen Rand verfügbar.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übersicht: *kafe*2\n",
    "***\n",
    "\n",
    "\n",
    "*kafe2* ist ist eine erweiterte Version des seit 2012 entwickelten Pakets *kafe* zur Anpassung\n",
    "von Modellfunktionen an Daten.\n",
    "\n",
    "Unterstützt werden verschiedene Datentypen wie einfache indizierte Daten, zweidimensionale\n",
    "Datenpunkte (eine Größe *x* und eine abhängige Größe *y*) sowie Häufigkeitsverteilungen\n",
    "(Histogramme).\n",
    "Unsicherheiten sowohl der abhängigen als auch der unabhängigen Größen und gegebenenfalls deren\n",
    "Korrelationen werden unterstützt.\n",
    "Dazu wird aus verschiedenen Arten von spezifizierten Unsicherheiten die globale Kovarianzmatrix\n",
    "erstellt und in der Anpassung berücksichtigt.\n",
    "Im Vergleich zu vielen anderen Anpassungswerkzeugen ist diese Möglichkeit ein\n",
    "Alleinstellungsmerkmal von *kafe(2)*.\n",
    "\n",
    "Unterstützt wird auch die gleichzeitige Anpassung mehrerer Modelle mit jeweils eigenen und\n",
    "zusätzlich allen oder mehreren Modellen zugehörigen Parametern an verschiedene Datensätze\n",
    "(\"Multi-Fit\").\n",
    "\n",
    "Zur Minimierung des Abstandsmaßes zwischen Daten und Modellfunktion(en) werden numerische\n",
    "Verfahren angewandt, die aus der quelloffenen, *Python*-basierten Softwareumgebung *SciPy* oder\n",
    "dem am CERN entwickelten Paket *MINUIT* stammen.\n",
    "Das jeweils minimierte Abstandsmaß (oder auch die \"Kostenfunktion\") entspricht dem mit einem\n",
    "Faktor Zwei multiplizierten negativen natürlichen Logarithmus der Likelihood-Funktion\n",
    "$-2\\,\\ln{\\cal L}$ der Daten für das gegebene Modell.\n",
    "Für Gauß-förmige Unsicherheiten der Datenpunkte entspricht dies der Methode der kleinsten\n",
    "Quadrate (auch \"$\\chi^2$-Methode\").\n",
    "Andere, auf dem Likelihood-Prinzip beruhende Kostenfunktionen für die Anpassung von\n",
    "Wahrscheinlichkeitsdichten an Histogramme oder an indizierte Daten sind ebenfalls verfügbar.\n",
    "\n",
    "Zur Bestimmung der Unsicherheiten auf die Parameter des angepassten Modells wird die Methode der\n",
    "Profil-Likelihood bereit gestellt, mit deren Hilfe Konfidenzintervalle für die einzelnen\n",
    "Parameter sowie zweidimensionale Konfidenz-Konturen für Paare von Parametern bestimmt werden\n",
    "können.\n",
    "\n",
    "*kafe2* enthält eine stand-alone Anwendung *kafe2go*, die Anpassungen ohne die Erstellung von\n",
    "eigenem Code ermöglicht;\n",
    "Daten, Modellfunktion und Optionen werden dazu in einer Konfigurationsdatei im *YAML*-Format angegeben.\n",
    "Mit dem folgenden Konsolenbefehl kann daraus ein Fit erstellt werden:\n",
    "\n",
    "`kafe2go <name>.yaml`\n",
    "\n",
    "*kafe2* kann aber ebenfalls und sehr viel flexibler über ein *Python*-Interface verwendet werden.\n",
    "Eine Einführung in die Möglichkeiten gibt dieses Tutorial.\n",
    "Für die häufigsten Anwendungsfälle gibt es in *kafe* vorkonfigurierte Pipelines.\n",
    "Für die Nutzung besagter Pipelines wird zuerst eine Funktion aufgerufen,\n",
    "die die numerische Anpassung durchführt, gefolgt von einer Funktion, die die Ergebnisse mit\n",
    "*matplotlib* grafisch darstellt.\n",
    "    \n",
    "**Die folgenden Beispiele zeigen die konkrete Vorgehensweise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allgemeine Einstellungen und nützliche Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function  # Python2-Kompatibilität\n",
    "import sys, os\n",
    "\n",
    "# Zeilen mit % oder %% am Anfang sind sogenannte \"magische Kommandos\",\n",
    "# die den Zellentyp oder Optionen für die Anzeige von Grafiken festlegen.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports und Voreinstellungen für *kafe2*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kafe2\n",
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# set better default figure size for kafe2\n",
    "# plt.rcParams['figure.figsize']=[12., 5.] \n",
    "#        !!!  must be done after importing kafe2 (will else be overwritten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Einfaches Beispiel zur Funktionsanpassung mit *kafe2*\n",
    "***\n",
    "\n",
    "Der folgende Code illustriert die Anpassung von Funktionen mit dem Anpassungswerkzeug *kafe2*:\n",
    "``` python\n",
    "# Define or read in the data for your fit:\n",
    "x_data = [1.0, 2.0, 3.0, 4.0]\n",
    "y_data = [2.3, 4.2, 7.5, 9.4]\n",
    "# x_data and y_data are combined depending on their order.\n",
    "# The above translates to the points (1.0, 2.3), (2.0, 4.2), (3.0, 7.5), and (4.0, 9.4).\n",
    "\n",
    "# Important: Specify uncertainties for the data!\n",
    "x_error = 0.1\n",
    "y_error = 0.4\n",
    "\n",
    "# Pass the information to kafe2:\n",
    "kafe2.xy_fit(x_data, y_data, x_error=x_error, y_error=y_error)\n",
    "# Because no model function was specified a line is used by default.\n",
    "\n",
    "# Call another function to create a plot:\n",
    "kafe2.plot(\n",
    "    x_label=\"x\",  # x axis label\n",
    "    y_label=\"y\",  # y axis label\n",
    "    data_label=\"Data\",  # label of data in legend\n",
    ")\n",
    "```\n",
    "\n",
    "Fügen Sie den Code in die leere Zelle unten ein und führen Sie sie durch Eingabe von `shift+return`\n",
    "aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einfaches Beispiel: Geradenanpassung mit kafe2\n",
    "# -> Code hier einfügen \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sollten zwei Grafiken sehen.\n",
    "Die erste Grafik zeigt die Daten, das Modell, und die Fit-Ergebnisse.\n",
    "Die zweite Grafik zeigt die sogenannte Profile-Likelihood der Modellparameter; darauf kommen wir später zurück.\n",
    "\n",
    "Die Grafiken und die Fit-Ergebnisse wurden ebenfalls als Dateien abgespeichert.\n",
    "Sehen Sie sich dazu den Unterordner `results` an.\n",
    "Er sollte nun zwei png-Dateien für die Grafiken, eine txt-Datei mit einem für Menschen lesbaren Bericht der Fit-Ergebnisse,\n",
    "und eine yml-Datei mit den Fit-Ergebnissen im *YAML*-Format enthalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Korrelierte Unsicherheiten\n",
    "***\n",
    "\n",
    "Zur Illustration der Möglichkeiten zur Behandlung von Unsicherheiten fügen wir eine weitere\n",
    "korrelierte Unsicherheit der abhängigen Größen *y* ein:\n",
    "``` python\n",
    "kafe2.xy_fit(x_data, y_data, x_error=x_error, y_error=y_error, y_error_cor=0.3)\n",
    "```\n",
    "Beachten Sie die Verwendung des Keyword-Arguments `y_error_cor` zur Definition der korrelierten Unsicherheit in *y*-Richtung.\n",
    "Modifizieren sie obigen Code, sodass die zusätzliche korrelierte Unsicherheit verwendet wird.\n",
    "Wiederholen Sie nun die Anpassung.\n",
    "\n",
    "Wie erwartet wirkt sich eine solche allen Datenpunkten gemeinsame Unsicherheit nicht auf die\n",
    "Steigung der Geraden, sondern nur auf den Parameter *b* aus.\n",
    "Dessen Unsicherheit wird nun größer - entsprechend der Wurzel aus der\n",
    "quadratischen Summe der Unsicherheiten von $\\pm 0.58$ aus der\n",
    "ursprünglichen Anpassung und der zusätzlichen korrelierten Unsicherheit von $\\pm 0.40$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code aus dem vorigen Beispiel kopieren und ergänzen\n",
    "# ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Wie Sie die Keyword-Argumente der *kafe2*-Funktionen herausfinden können\n",
    "\n",
    "Es ist offensichtlich möglich, in der Funktion `kafe2.xy_fit` korrelierte Unsicherheiten zu definieren.\n",
    "Notwendigerweise muss Ihnen dafür jedoch das Keyword-Argument `y_error_cor` bekannt sein.\n",
    "Wie hätten Sie sich dieses Wissen also selbst aneignen können?\n",
    "Die erste Möglichkeit liegt darin, die *kafe2*-Dokumentation zu lesen, in der alle für Nutzer relevanten\n",
    "Keyword-Argumente beschrieben werden.\n",
    "Alternativ dazu kann diese Information auch direkt in einem Jupyter-Notebook durch die eingebaute Hilfefunktion abgerufen werden.\n",
    "Information über die Funktion `kafe2.plot` können Sie zum Beispiel erhalten, indem Sie den folgenden Text in die darunterliegende Zelle kopieren:\n",
    "``` python\n",
    "? kafe2.plot\n",
    "```\n",
    "Die Ausgabe der Jupyter-Hilfefunktion erfolgt in mehreren Teilen.\n",
    "Zuerst wird die Signatur der Funktion sowie die Standardwerte der Argumente ausgegeben\n",
    "(für `kafe2.plot` bedeutet der Wert `None`, dass der tatsächlich verwendete Wert vom Fit abgeleitet werden soll).\n",
    "Anschließend wird der \"Docstring\" der Funktion ausgegeben, welcher die Bedeutung und die erwarteten Datentypen der Argumente beschreibt.\n",
    "Zuletzt wird die Datei angegeben, welche den Quellcode der Funktion enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopieren Sie einfach \"? kafe2.plot\" (ohne Anführungszeichen) hierher, um Hilfe zu erhalten\n",
    "# ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Vergleich von zwei verschiedenen Modellen \n",
    "***\n",
    "\n",
    "Auf kleinen Skalen kann aufgrund der Taylorentwicklung jede Funktion mit einer Geraden angenähert werden.\n",
    "Für viele Problemstellungen ist diese Näherung jedoch unzureichend.\n",
    "Das folgende Beispiel zeigt die Anpassung eines linearen und eines exponentiellen Modells an die\n",
    "gleichen Daten.\n",
    "\n",
    "Um eine Modellfunktion für *kafe2* zu definieren, genügt es, eine *Python*-Funktion\n",
    "zu schreiben.\n",
    "Wichtig:\n",
    "das erste Argument der *Python*-Funktion wird als unabhängige Variable interpretiert.\n",
    "Das erste Argument wird während der Anpassung also nicht modifiziert und es ist die Größe,\n",
    "die vom Fit als x-Achse interpretiert wird.\n",
    "\n",
    "Definition von zwei Modellfunktionen:\n",
    "``` python\n",
    "# Our first model is a simple linear function:\n",
    "def linear_model(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "# Our second model is a simple exponential function.\n",
    "# The kwargs in the function header specify parameter defaults.\n",
    "def exponential_model(x, A_0=1., x_0=5.):\n",
    "    return A_0 * np.exp(x/x_0)\n",
    "```\n",
    "\n",
    "Hier die Definition der Daten:\n",
    "``` python\n",
    "# The data for this exercise:\n",
    "x_data_2 = [19.8, 3.0, 5.1, 16.1, 8.2, 11.7, 6.2, 10.1]\n",
    "y_data_2 = [23.2, 3.2, 4.5, 19.9, 7.1, 12.5, 4.5, 7.2]\n",
    "x_error_2 = 0.3\n",
    "y_error_rel_2 = 0.15\n",
    "```\n",
    "\n",
    "Um zwei Anpassungen durchzuführen, muss die Funktion `kafe2.xy_fit` zwei mal aufgerufen werden:\n",
    "``` python\n",
    "results_linear = kafe2.xy_fit(\n",
    "    x_data_2, y_data_2, model_function=linear_model, \n",
    "    x_error=x_error_2, y_error_rel=y_error_rel_2)\n",
    "results_exponential = kafe2.xy_fit(\n",
    "    x_data_2, y_data_2, model_function=exponential_model,\n",
    "    x_error=x_error_2, y_error_rel=y_error_rel_2, profile=True)\n",
    "```\n",
    "Die Fit-Ergebnisse werden als `results_linear` und `results_exponential` abgespeichert (Sie werden sie später noch brauchen).\n",
    "\n",
    "Achten Sie darauf, `profile=True` anzugeben, wenn Sie eine nichtlineare Modellfunktion verwenden.\n",
    "Eine Modellfunktion ist genau dann linear, wenn sie für jeden ihrer Modellparameter einzeln eine lineare Funktion ist.\n",
    "Die Modellfunktion muss keine lineare Funktion der unabhängigen Variable *x* sein.\n",
    "Beispiele: alle polynomiellen Modellfunktionen sind linear, trigonometrische Funktionen sind nichtlinear.\n",
    "\n",
    "Wenn Sie nun wie zuvor einfach `kafe2.plot` aufriefen, würde nur eine Grafik des exponentiellen Fits erstellt werden.\n",
    "Dies liegt daran, dass `kafe2.plot` standardmäßig nur eine Grafik der zuletzt durchgeführten Anpassung erstellt.\n",
    "Übergeben Sie `-2` als erstes Argument, um eine Grafik der letzten zwei Anpassungen zu erstellen:\n",
    "``` python\n",
    "kafe2.plot(\n",
    "    -2,\n",
    "    # parameter_names=dict(x=\"t\", a=r\"\\alpha\", b=r\"\\beta\", A_0=\"I_0\", x_0=\"t_0\"),\n",
    "    model_expression=[\"{a}{x} + {b}\", \"{A_0} e^{{ {x} / {x_0} }}\"]\n",
    ")\n",
    "```\n",
    "Es ist im Allgemeinen nicht möglich, den Python-Code für Modellfunktionen in eine gültige LaTeX-Darstellung zu übersetzen.\n",
    "Das Keyword-Argument `model_expression` übergibt diese Information an `kafe2.plot`.\n",
    "Die Parameternamen müssen dazu mit {} umschlossen werden.\n",
    "Um {} für LaTeX-Syntax zu erhalten, müssen sie als {{ und }} doppelt geschrieben werden.\n",
    "\n",
    "Sie können außerdem das auskommentierte Keyword-Argument `parameter_names` verwenden, um die Namen der Modellparameter zu ändern.\n",
    "\n",
    "Fügen Sie den Code in die leere Zelle unten ein und führen Sie sie durch Eingabe von\n",
    "`shift+return` aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich von zwei Modellen mit kafe2\n",
    "# -> code hier einfügen \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es sollten drei Grafiken zu sehen sein.\n",
    "Die erste Grafik zeigt die Daten, die Modelle, und die Ergebnisse der beiden Anpassungen.\n",
    "Die anderen beiden Grafiken zeigen die Profile-Likelihood des linearen und des exponentiellen Fits.\n",
    "\n",
    "Im Idealfall ist die Modellfunktion linear und es liegen nur konstante Unsicherheiten in y-Richtung vor.\n",
    "Die Ergebnisse, die mit der Methode der kleinsten Fehlerquadrate ($\\chi^2$-Methode) ermittelt werden,\n",
    "sind dann optimal und unverzerrt.\n",
    "Die konventionellen \"parabolischen\" Parameterunsicherheiten der Art $a \\pm \\Delta a$ sind dann akkurat.\n",
    "\n",
    "Im zuvor genannten Idealfall ist die Profile-Likelihood eines einzelnen Parameters (das \"Profil\")\n",
    "eine Parabel;\n",
    "Die Profile-Likelihood eines Paares von zwei Parametern ist ein Paraboloid.\n",
    "Da ein Paraboloid dreidimensional ist, kann er nicht direkt in zwei Dimensionen dargestellt werden.\n",
    "Stattdessen wird die Schnittmenge der Profile-Likelihood mit horizontalen Ebenen überhalb des Minimums\n",
    "der Kostenfunktion (die \"Konturen\") angezeigt.\n",
    "Im Idealfall haben die Konturen die Form von Ellipsen.\n",
    "\n",
    "Die Profile-Likelihood des exponentiellen Fits weicht deutlich von der Form einer Parabel/Ellipse ab.\n",
    "Dies ist nicht verwunderlich, da das exponentielle Modell keine lineare Funktion des Parameters $x_0$ ist.\n",
    "Bei genauer Betrachtung fällt jedoch auf, dass die Profile-Likelihood auch unter Verwendung eines linearen\n",
    "Modells verzerrt ist.\n",
    "Die Gründe werden in den folgenden Abschnitten erklärt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Verzerrungen aufgrund von Unsicherheiten in x-Richtung\n",
    "***\n",
    "\n",
    "Wenn Unsicheheiten in x-Richtung verwendet werden, dann wird die\n",
    "Anpassung einer Geraden ebenfalls zu einem nichtlinearen Problem.\n",
    "Die Ursache hierfür liegt darin, dass die x-Fehler während der\n",
    "Anpassung durch Multiplikation mit der Ableitung der Modellfunktion\n",
    "nach *x* in y-Fehler umgewandelt werden.\n",
    "Der Gesamtfehler wird dadurch also zu einer Funktion der Modellparameter\n",
    "und aus einer linearen Regression wird ein nichtlineares Problem.\n",
    "Wiederholen Sie zur Veranschaulichung den linearen Fits mit deutlich größeren\n",
    "Unsicherheiten in x-Richtung:\n",
    "``` python\n",
    "kafe2.xy_fit(x_data_2, y_data_2, x_error=4*x_error_2, y_error_rel=y_error_rel_2)\n",
    "kafe2.plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code von oben kopieren und hier einfügen\n",
    "# ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der zweiten Grafik sollte zu sehen sein, dass die Profile-Likelihood der Parameter durch eine\n",
    "Erhöhung der x-Fehler deutlich asymmetrischer geworden ist.\n",
    "Der Grund dafür liegt darin, dass hohe Werte des Parameters $a$ in einer hohen Ableitung der Modellfunktion\n",
    "und somit höheren Gesamtfehlern resultieren, was wiederherum den Wert der Kostenfunktion senkt.\n",
    "Im Gegenzug sorgen geringe Werte für $a$ zu einer niedrigen Ableitung, niedrigeren Gesamtfehlern,\n",
    "und hohen Werten der Kostenfunktion.\n",
    "Der Parameter $b$ wird zwar nicht direkt von x-Fehlern beinflusst, er wird duch seine negative Korrelation\n",
    "mit $a$ jedoch auch indirekt beeinflusst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Verzerrungen aufgrund von relativen Unsicherheiten in y-Richtung\n",
    "***\n",
    "\n",
    "Relative y-Fehler verzerren mit den *kafe2* gewählten Voreinstellungen ebenfalls die Profile-Likelihood\n",
    "Im Idealfall würden diese y-Fehler relativ zu den wahren y-Werten berechnet werden.\n",
    "Da die wahren Werte jedoch unbekannt sind, werden die Fehler stattdessen relativ zum Modell berechnet.\n",
    "Die y-Fehler werden dadurch parameterabhängig, wodurch das Regressionproblem nichtlinear wird.\n",
    "\n",
    "In einem alternativen Ansatz können die y-Fehler relativ zu den Daten berechnet werden (mit `errors_rel_to_model=False`).\n",
    "Die Regression ist dann weiterhin linear, aber die Fit-Ergebnisse werden verzerrt.\n",
    "Die Ursache der Verzerrung liegt darin, dass Datenpunkten, die zufällig zu kleineren Werten fluktuieren,\n",
    "eine geringere Unsicherheit zugewiesen wird, als Datenpunkten, die zufällig zu größeren Werten fluktuieren.\n",
    "\n",
    "Der folgende Code kann für einen Vergleich der zwei Modi verwendet werden:\n",
    "``` python\n",
    "kafe2.xy_fit(x_data_2, y_data_2, x_error=x_error_2, \n",
    "             y_error_rel=4*y_error_rel_2, errors_rel_to_model=True)\n",
    "kafe2.xy_fit(x_data_2, y_data_2, x_error=x_error_2, \n",
    "             y_error_rel=4*y_error_rel_2, errors_rel_to_model=False)\n",
    "kafe2.plot(-2)\n",
    "```\n",
    "Die Voreinstellung ist `errors_rel_to_model=True`.\n",
    "Die Angabe im ersten Fall kann also weggelassen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code von oben kopieren und hier einfügen\n",
    "# ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Ausgabe der Anpassungsergebnisse als Variable \n",
    "***\n",
    "\n",
    "In vielen Anwendungen ist es nötig, das Ergebnis einer Anpassung im Programmcode weiter zu verwenden.\n",
    "Dazu kann der Rückgabewert von `kafe2.xy_fit`, ein *Python*-Dictionary mit den Fit-Ergebnissen, verwendet werden.\n",
    "In einem vorigen Abschnitt wurden zum Beispiel Ergnisse als `results_linear` und `results_exponential` gespeichert.\n",
    "\n",
    "Eine formatierte Ausgabe der Fit-Ergebnisse kann mit foldendem Einzeiler erreicht werden:\n",
    "``` python\n",
    "print(\"\\n\\n\".join(f\"====== {k} ======\\n{v}\" for k, v in results_linear.items()))\n",
    "```\n",
    "Bei einer direkten Verwendung von *kafe2*-Objekten können diese Werte auch über die Properties der Objekte abgerufen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe hier testen \n",
    "# --> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Hypothesentest zur Bewertung der Modelle\n",
    "***\n",
    "\n",
    "Die grafische Ausgabe lässt nicht klar erkennen, welches der Modelle akzeptabel ist. \n",
    "Dazu kann ein Hypothesentest ausgeführt werden, der die sogenannte $\\chi^2$-Wahrscheinlichkeit\n",
    "angibt - also die Wahrscheinlichkeit dafür, einen schlechteren Wert von\n",
    "$\\chi^2$ am Minimum zu erhalten als den beobachteten.\n",
    "Ein höherer Wert entspricht einem besseren Fit.\n",
    " \n",
    "Berechnet wird der Wert aus der Verteilungsfunktion der $\\chi^2$-Verteilung:\n",
    "``` python\n",
    "from scipy import stats\n",
    "\n",
    "def chi2prob(chi2, ndf):\n",
    "  \"\"\" chi2-probability\n",
    " \n",
    "    Args:\n",
    "      * chi2: chi2 value\n",
    "      * ndf: number of degrees of freedom\n",
    "\n",
    "    Returns:\n",
    "      * float: chi2 probability\n",
    "  \"\"\"\n",
    "\n",
    "  return 1.- stats.chi2.cdf(chi2, ndf)\n",
    "```\n",
    "\n",
    "Geben Sie den Code für die $\\chi^2$-Wahrscheinlichkeit in die leere Zelle unten ein und\n",
    "überprüfen Sie die beiden Ergebnisse, die Sie oben erhalten haben.\n",
    "\n",
    "**Hinweis**: Sie können die Werte für $\\chi^2$ und die Zahl der Freiheitsgrade entweder\n",
    "aus der Ausgabe der vorigen Zellen abtippen oder Sie können die Werte über die Properties `goodness_of_fit` und `ndf`\n",
    "der als `results_linear` und `results_exponential` gespeicherten Fit-Ergebnisse beziehen.\n",
    "Das Property `cost_function_value` ist nicht geeignet, da es neben $\\chi^2$\n",
    "auch Korrekturterme enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfung der Qualität der Anpassungen\n",
    "# -> code hier eingeben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Anwendung von *kafe2* außerhalb dieses Notebooks empfiehlt es sich, die\n",
    "$\\chi^2$-Wahrscheinlichkeit einfach über das entsprechende Property der Fit-Ergebnisse zu beziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Modelldefinition per SymPy\n",
    "\n",
    "Wenn *SymPy* (Symbolic Python) installiert ist, kann dieses Programmpaket zur Definition\n",
    "von Modellfunktionen verwendet werden.\n",
    "Die oben verwendeten linearen und exponentiellen Modelle können zum Beispiel so definiert werden:\n",
    "``` python\n",
    "linear_model = \"linear_model: x a b -> a * x + b\"\n",
    "exponential_model = \"exponential_model: x A_0 x_0=5.0 -> A_0 * exp(x / x_0)\"\n",
    "```\n",
    "Die Zeichenfolge `->` trennt die Definition der Argumente von der Definition des Funktionsausdrucks.\n",
    "Der Anfang der Definition bis `:` definiert den namen der Modellfunktion (kann weggelassen werden).\n",
    "\n",
    "Der Vorteil einer Definition mit *SymPy* liegt darin, dass *kafe2* dann automatisert LaTeX-Darstellungen\n",
    "der Modellfunktion generieren kann.\n",
    "Wiederholen Sie die Anpassung der zwei Modelle mit den obigen Definitionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anpassung mit per SymPy definierten Modellen\n",
    "# -> code hier eingeben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Beeinflussung der grafischen Ausgabe\n",
    "***\n",
    "\n",
    "Die grafische Ausgabe war noch nicht in allen Belangen optimal. \n",
    "Es wurden in der Legende z.B. zwei Datensätze angegeben, obwohl es für beide Modelle nur den\n",
    "gleichen gab.\n",
    "Außerdem lassen sich Markereigenschaften und Farben anpassen.\n",
    "Dies ist allerdings etwas komplizierter als z.B. die Anpassung der Achsenbeschriftungen.\n",
    "Die Funktion `kafe2.plot` hat keine Argumente für Farben.\n",
    "Stattdessen muss das Objekt `kafe2.Plot` (großgeschrieben) verwendet werden:\n",
    "``` python\n",
    "p = kafe2.Plot(-2)\n",
    "```\n",
    "Zur Beeinflussung der Grafik enhält *kafe2* eine Methode `Plot.customize`, mit deren Hilfe für\n",
    "die verschiedenen Grafikelemente (*plot_types*: 'data', 'model_line', 'model_error_band',\n",
    "'ratio', 'ratio_error_band') Werte für *matplotlib*-Parameter angegeben werden können.\n",
    "\n",
    "Die für einen *plot_type* relevanten Parameter und deren momentane Werte lassen sich über eine\n",
    "Funktion der *Plot*-Klasse anzeigen:\n",
    "``` python\n",
    "p.get_keywords('model_error_band')\n",
    "```\n",
    "\n",
    "Die verwendeten Namen für Objekte und mögliche Werte entstprechen den Bezeichnungen in der\n",
    "Konfigurationsdatei *matplotlibrc* für *matplotlib*.\n",
    "\n",
    "Zur Änderung des Namens für den Datensatz und die Unterdrückung der zweiten Ausgabe dient\n",
    "folgender Aufruf:\n",
    "``` python\n",
    "p.customize('data', 'label', [\"test data\", None])\n",
    "```\n",
    "Das erste Argument spezifiziert den Teil des Plots, der modifiziert werden soll.\n",
    "Das zweite Argument spezifiziert welches Keyword gesetzt werden soll.\n",
    "Das dritte Argument ist eine Liste mit Werten, die jeweils für das Keyword bei den vom\n",
    "Plot-Objekt verwalteten Fit-Objekten gesetzt werden sollen.\n",
    "\n",
    "Alternativ kann für das dritte Argument auch eine Liste an Tupeln aus Fit-Indices und\n",
    "Werten übergeben werden:\n",
    "``` python\n",
    "p.customize('data', 'label', [(0, \"test data\"), (1, None)])\n",
    "```\n",
    "Mit dieser Syntax genügt es, nur für einen Teil der Fits Werte anzugeben.\n",
    "\n",
    "Auch Marker-Typ, Größe und Farbe des Markers und der Fehlerbalken lassen sich anpassen:\n",
    "``` python\n",
    "# data\n",
    "p.customize('data', 'marker', ['o', 'o'])\n",
    "p.customize('data', 'markersize', [5, 5])\n",
    "p.customize('data', 'color', [(0, 'blue'), (1,'blue')]) # note: although 2nd label is suppressed\n",
    "p.customize('data', 'ecolor', [(0, 'blue'), (1, 'blue')]) # note: although 2nd label is suppressed\n",
    "```\n",
    "\n",
    "Ebenso können die entsprechenden Werte für die Modellfunktion angepasst werden:\n",
    "``` python\n",
    "# model\n",
    "p.customize('model_line', 'color', ['orange', 'lightgreen'])\n",
    "p.customize('model_error_band', 'label', [(0, r'$\\pm 1 \\sigma$'), (1, r'$\\pm 1 \\sigma$')])\n",
    "p.customize('model_error_band', 'color', [(0, 'orange')])\n",
    "p.customize('model_error_band', 'color', [(1, 'lightgreen')])\n",
    "```\n",
    "\n",
    "Es ist auch möglich, Parameter über die *matplotlib*-Funktionen zu verändern. \n",
    "Um die Größe der Achsenbeschriftungen zu ändern, verwendet man z.B. folgende Aufrufe:\n",
    "``` python\n",
    "# Größe der Achsenbeschriftungen\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=20, titlesize=25)\n",
    "```\n",
    "Achtung: der obige Aufruf führt zu einer globalen Änderung der *matplotlib*-Parameter.\n",
    "Plots außerhalb von *kafe2* werden also auch beeinflusst.\n",
    "\n",
    "Mit dem *kafe2* Plot-Objekt müssen die Grafiken manuell erstellt und angezeigt werden:\n",
    "``` python\n",
    "p.plot()\n",
    "p.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code zum Testen hier eingeben:\n",
    "# --> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kleine Übung: Bessere Parametrisierung des exponentiellen Modells\n",
    "\n",
    "Die Fit-Ergebnisse des exponentiellen Modells können durch eine Umparametrisierung verbessert werden: $f(x; A_0, \\lambda) = A_0 e^{\\lambda x}$.\n",
    "Passen Sie das Modell mit der alternativen Parametrisierung an die Daten an und stellen sie die Ergebnisse\n",
    "zusammen mit denen des linearen und des ursprünglichen exponentiellen Fits dar.\n",
    "\n",
    "**Hinweis**: der Name `lambda` ist in *Python* für die Definition anonymer Funktionen reserviert.\n",
    "Sie müssen deshalb einen anderen Namen zur Definition der Modellfunktion verwenden.\n",
    "Der Name `varlambda` wird in *kafe2* automatisch als der griechische Buchstabe lambda dargestellt.\n",
    "Grundsätzlich können sie jedoch einen beliebigen Namen verwenden, solange Sie beim Plotten `parameter_names` setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenen Code hier eingeben:\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sollten sehen, dass sich die Fit-Ergebnisse durch die Umparametrisierung quasi nicht geändert haben.\n",
    "Das Profil von $\\lambda$ sollte im Gegensatz zum Profil von $x_0$ nahezu parabolisch sein.\n",
    "Die konventionelle Beschreibung mit Wert und Standardabweichung $\\lambda \\pm \\Delta \\lambda$ ist also\n",
    "deutlich akkurater als die äquivalente Beschreibung $x_0 \\pm \\Delta x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Einschub: Objektorientierte Programmierung\n",
    "---\n",
    "Bis jetzt wurden die Modellanpassungen mit den Funktionen `kafe2.xy_fit` und `kafe2.plot` durchgeführt.\n",
    "Diese Funktionen stellen vorkonfigurierte Pipelines für die Anpassung von Modellen an xy-Daten sowie\n",
    "für die Darstellung der Fit-Ergebnisse zur Verfügung.\n",
    "Intern verwenden diese Funktionen Objekte (im Sinne der objektorientierten Programmierung) welche\n",
    "beispielsweise Daten oder einen Fit als Ganzes repräsentieren.\n",
    "Von nun an werden die Beispiele die Objekte direkt verwenden.\n",
    "Das allgemeine Vorgehen ist:\n",
    "\n",
    "1. Ein Container-Objekt, welches Daten und die zugehörigen Unsicherheiten enthält, wird erzeugt.\n",
    "2. Ein Fit-Objekt wird von einem Container-Objekt (oder Rohdaten) und einer Modellfunktion erzeugt.\n",
    "   Äquivalent zu `kafe2.xy_fit` wird standardmäßig eine Gerade als Modellfunktion verwendet.\n",
    "3. Die Methode `do_fit` des Fit-Objekts wird aufgerufen, um die numerische Minimierung der Kostenfunktion durchzuführen.\n",
    "4. Die Fit-Ergebnisse werden extrahiert. Verwenden Sie dazu entweder die Properties des Fit-Objekts, geben Sie einen\n",
    "   Bericht auf der Konsole aus, oder erstellen Sie eine Grafik.\n",
    "\n",
    "Die folgende Implemeniterung ist grob äquivalent zum ersten Beispiel, in dem eine Gerade angepasst wurde.\n",
    "Zuerst wird ein *XYContainer*-Objekt aus den Daten erstellt.\n",
    "Unsicherheiten werden über die Methode `XYContainer.add_error` zum Container hinzugefügt.\n",
    "``` python\n",
    "xy_data = XYContainer(x_data, y_data)\n",
    "xy_data.add_error('x', x_error)\n",
    "xy_data.add_error('y', y_error)\n",
    "xy_data.label = 'Data'  # How the data is called in plots\n",
    "```\n",
    "Als Nächstes wird das Fit-Objekt aus dem Daten-Container erstellt.\n",
    "In diesem Fall wird die Methode `do_fit` sofort aufgerufen, aber es ist möglich,\n",
    "zuerst z.B. Parameter auf Intervalle zu beschränken oder weitere Unsicherheiten zu definieren.\n",
    "``` python\n",
    "line_fit = Fit(data=xy_data)\n",
    "line_fit.do_fit()  # This will throw a warning if no errors were specified.\n",
    "```\n",
    "Zuletzt werden die Fit-Ergebnisse extrahiert.\n",
    "Die *Plot*-Klasse ist die gleiche wie im vorigen Beispiel.\n",
    "``` python\n",
    "line_fit.report()  # Prints fit results to console.\n",
    "\n",
    "plot = Plot(fit_objects=line_fit)  # Create a kafe2 plot object.\n",
    "plot.x_label = 'x'  # Set x axis label.\n",
    "plot.y_label = 'y'  # Set y axis label.\n",
    "plot.plot()  # Do the plot.\n",
    "plot.show()  # Just a convenience wrapper for matplotlib.pyplot.show() .\n",
    "```\n",
    "Beachten Sie, dass die Fit-Ergebnisse **nicht** automatisch als Dateien abgespeichert werden.\n",
    "Zum Abspeichern der Grafik kann `plot.save()` zu obigem Code hinzugefügt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafe2 import XYContainer, Fit, Plot\n",
    "# Obigen Code hier einfügen\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Besonderheiten komplexer (nichtlinearer) Modelle\n",
    "---\n",
    "\n",
    "Als weiteres Beispiel für eine nicht-lineare Anpassung dient eine gedämpfte Schwingung eines\n",
    "Fadenpendels.\n",
    "Die zugehörigen Messdaten sind in der folgenden Code-Zelle enthalten:\n",
    "``` python\n",
    "# The data:\n",
    "t = [ ... ]\n",
    "t_errors = 0.05\n",
    "\n",
    "a = [ ... ]\n",
    "a_errors = 0.05\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data:\n",
    "t = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0,\n",
    "     10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0, 14.5, 15.0, 15.5, 16.0, 16.5, 17.0, 17.5, 18.0, 18.5, 19.0,\n",
    "     19.5, 20.0, 20.5, 21.0, 21.5,22.0, 22.5, 23.0, 23.5, 24.0, 24.5, 25.0, 25.5, 26.0, 26.5, 27.0, 27.5, 28.0,\n",
    "     28.5, 29.0, 29.5, 30.0, 30.5, 31.0, 31.5, 32.0, 32.5, 33.0, 33.5, 34.0, 34.5, 35.0, 35.5, 36.0, 36.5, 37.0,\n",
    "     37.5, 38.0, 38.5, 39.0, 39.5, 40.0, 40.5, 41.0, 41.5, 42.0, 42.5, 43.0, 43.5, 44.0, 44.5, 45.0, 45.5, 46.0,\n",
    "     46.5, 47.0, 47.5, 48.0, 48.5, 49.0, 49.5, 50.0, 50.5, 51.0, 51.5, 52.0,52.5, 53.0, 53.5, 54.0, 54.5, 55.0,\n",
    "     55.5, 56.0, 56.5, 57.0, 57.5, 58.0, 58.5, 59.0, 59.5, 60.0]\n",
    "t_errors = 0.05\n",
    "\n",
    "a = [ 6.06,  5.17,  3.29,  0.64, -2.26, -4.56, -5.74, -5.58, -4.12, -1.62,\n",
    "      1.11,  3.56,  5.12,  5.43,  4.41,  2.53, -0.18, -2.78, -4.65, -5.5,\n",
    "     -5.04, -3.25, -0.75,  1.79,  3.88,  5.31,  5.2,   3.92,  1.74, -0.85,\n",
    "     -3.13, -4.71, -5.06, -4.26, -2.48, -0.13,  2.19,  4.07,  4.9,   4.64,\n",
    "      3.16,  1.17, -1.54, -3.26, -4.59, -4.64, -3.69, -1.83,  0.38,  2.76,\n",
    "      4.16,  4.58,  4.13,  2.45,  0.28, -1.8,  -3.53, -4.43, -4.31, -3.03,\n",
    "     -1.05,  1.06,  2.79,  3.97,  4.4,   3.37,  1.92, -0.14, -2.29, -3.7,\n",
    "     -4.28, -3.84, -2.44, -0.59,  1.27,  3.11,  3.9,   4.02,  2.85,  1.21,\n",
    "     -0.64, -2.51, -3.41, -3.84, -3.34, -1.75, -0.17,  1.85,  3.23,  3.72,\n",
    "      3.4,   2.54,  0.67, -1.13, -2.8,  -3.77, -3.65, -2.89, -1.43,  0.42,\n",
    "      2.2,   3.26,  3.42,  3.25,  1.88,  0.33, -1.35, -3.02, -3.41, -3.32,\n",
    "     -2.2,  -0.77,  0.92,  2.44,  3.31,  3.44,  2.77,  1.25, -0.13, -1.69, -2.78 ]\n",
    "a_errors = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Amplitude in Abhängigkeit von der Zeit ist durch folgende Modellfunktion gegeben:\n",
    "``` python\n",
    "# Model function for a pendulum as a one-dimensional,\n",
    "#     damped harmonic oscillator with zero initial speed:\n",
    "# x = time, y_0 = initial_amplitude, l = length of the string,\n",
    "# r = radius of the steel ball, g = gravitational acceleration, c = damping coefficient.\n",
    "def damped_harmonic_oscillator(s, a0, l, r, g, c):\n",
    "  # Effective length of the pendulum = length of the string + radius of the steel ball:\n",
    "  l_total = l + r\n",
    "  omega_0 = np.sqrt(g / l_total) # Phase speed of an undamped pendulum.\n",
    "  omega_d = np.sqrt(omega_0 ** 2 - c ** 2) # Phase speed of a damped pendulum.\n",
    "  return a0 * np.exp(-c * s) * (np.cos(omega_d * s) + c / omega_d * np.sin(omega_d * s))\n",
    "```\n",
    "\n",
    "Daten-Container und Fit-Objekt werden wie üblich erzeugt:\n",
    "``` python\n",
    "# Create data container:\n",
    "data3 = XYContainer(t, a)\n",
    "data3.add_error(axis='x', err_val=t_errors)\n",
    "data3.add_error(axis='y', err_val=a_errors)\n",
    "data3.axis_labels = ('Time t (s)', 'Amplitude A (°)') \n",
    "\n",
    "# Create fit object from data and model function:\n",
    "fit = Fit(data3, damped_harmonic_oscillator)\n",
    "```\n",
    "\n",
    "Das Modell enthält eine Anzahl an Parametern, die durch \"Hilfsmessungen\" festgelegt sind. \n",
    "``` python\n",
    "# Relevant physical magnitudes and their uncertainties:\n",
    "lm, delta_lm = 10.000, 0.002  # length of the string, l = 10.0 +- 0.002 m\n",
    "rm, delta_rm = 0.052, 0.001  # radius of the steel ball, r = 0.052 +- 0.001 m\n",
    "# Amplitude of the steel ball at x=0 in degrees, a0m = 6 +- 1% degrees:\n",
    "a0m, delta_a0m = 6.0, 0.01  # Note that the uncertainty on a0m is relative to a0m.\n",
    "```\n",
    "\n",
    "In der Anpassung wird dies berücksichtigt, indem die entsprechenden Parameter sowohl als\n",
    "Parameter der Anpassung als auch als zusätzliche Datenpunkte berücksichtigt werden.\n",
    "In kafe2 werden solche durch Messungen eingeschränkte Parameter mit Hifle der Methode\n",
    "*Fit.add_parameter_constraint()* berücksichtigt und deren Unsicherheiten in das Ergebnis der\n",
    "Anpassung propagiert:\n",
    "``` python\n",
    "# Constrain model parameters to measurements:\n",
    "fit.add_parameter_constraint(name='l', value=lm, uncertainty=delta_lm)\n",
    "fit.add_parameter_constraint(name='r', value=rm, uncertainty=delta_rm)\n",
    "fit.add_parameter_constraint(name='a0', value=a0m, uncertainty=delta_a0m, relative=True)\n",
    "```\n",
    "Als Alternative könnte man die Parameter mit der Methode *Fit.fix_parameter()* auf feste Werte\n",
    "fixieren;\n",
    "die Unsicherheiten auf das Endergebnis der Anpassung müssten dann allerdings mit Hilfe der\n",
    "klassischen Fehlerfortpflanzung berechnet werden.\n",
    "\n",
    "Als weitere Besonderheit bei nichtlinearen Anpassungen ist zu beachten, dass es häufig\n",
    "Nebenminima der Kostenfunktion gibt - die Konvergenz zum globalen Minimum kann also nicht\n",
    "garantiert werden.\n",
    "Es ist daher notwendig, \"vernünftige\" Start-Parameter für die Anpassung zu wählen.\n",
    "Dies geschieht mit Hilfe der Funktion *Fit.set_parameter_values()*:\n",
    "``` python\n",
    "g_initial = 9.81  # Initial guess for g.\n",
    "fit.set_parameter_values(g=g_initial, a0=a0m, l=lm, r=rm)\n",
    "```\n",
    "Wenn die Startwerte gänzlich unbekannt sind, sollten Werte in einem großen Bereich ausprobiert\n",
    "werden, um zu überprüfen, ob die Anpassungen jeweils zum gleichen Minimum konvergieren. \n",
    "\n",
    "Ein weiteres Mittel zur Verbesserung der Konvergenz liegt in der Beschränkung der Parameter\n",
    "auf \"vernünftige\" Intervalle.\n",
    "Die Parameter *a0*, *l*, und *r* sind zum Beispiel per Definition positiv, können während\n",
    "der Anpassung jedoch auch negative Werte annehmen.\n",
    "Der folgende Code beschränkt die erwähnten Parameter auf positive Werte:\n",
    "``` python\n",
    "fit.limit_parameter(\"a0\", lower=1e-6)\n",
    "fit.limit_parameter(\"l\", lower=1e-6)\n",
    "fit.limit_parameter(\"r\", lower=1e-6)\n",
    "```\n",
    "Aus technischen Gründen können Parameter nur auf geschlossene Intervalle beschränkt werden.\n",
    "Als untere Grenze wird hier deshalb ein kleiner Wert nahe null angegeben.\n",
    "Da kein oberer Wert angegeben wird, sind die Parameter nur einseitig beschränkt.\n",
    "Es ist auch möglich, Parameter durch die Angabe von zwei Intervallgrenzen enger einzugrenzen:\n",
    "``` python\n",
    "fit.limit_parameter(\"g\", lower=9.71, upper=9.91)\n",
    "```\n",
    "Im obigen Falle beruht die Beschränkung auf der Einschätzung, dass Ergebnisse außerhalb\n",
    "dieser Grenzen sehr unwahrscheinlich sind.\n",
    "Es macht auch Sinn, Parameter basierend auf den physikalischen Gegebenheiten des Systems\n",
    "zu beschränken.\n",
    "Zum Beispiel liefert die Modellfunktion nur für $c < \\frac{g}{l + r}$ reelle Lösungen.\n",
    "Man kann dies folgendermaßen berücksichtigen:\n",
    "``` python\n",
    "c_max = 0.9 * g_initial / (lm + rm)  # A little lower than our best guess for the limit.\n",
    "fit.limit_parameter(\"c\", lower=1e-6, upper=c_max)\n",
    "```\n",
    "\n",
    "Nach diesen Vorbereitungen kann die Anpassung wie üblich vorgenommen werden. \n",
    "Im folgenden Code-Beispiel wird auch gezeigt, wie man über Properties\n",
    "auf die Fit-Ergebnisse zugreift, fall sie im Programm weiter verarbeitet werden sollen\n",
    "oder eine spezifische eigene Ausgabe erfolgen soll.\n",
    "``` python\n",
    "# Perform the fit\n",
    "fit.do_fit()\n",
    "# Optional: Print out a report on the fit results on the console.\n",
    "#fit.report(show_data=False, show_model=False, show_fit_results=True)\n",
    "\n",
    "# Custom printout of results:\n",
    "print(\"cost function at minimum: %.4g \" % fit.cost_function_value,\n",
    "    \" number of degrees of freedom:\", fit.ndf)\n",
    "print(\" --> probability: %.1f%%\" % (fit.chi2_probability * 100))\n",
    "print(\"parameter names:\\n\", fit.parameter_names)\n",
    "np.set_printoptions(precision=5, suppress=False)\n",
    "print(\"prameter values:\\n\", fit.parameter_values)\n",
    "print(\"parameter uncertainties:\\n\",fit.parameter_errors)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"correlation matrix:\\n\", fit.parameter_cor_mat )\n",
    "      \n",
    "# Optional: plot the fit results.\n",
    "plot = Plot(fit)\n",
    "plot.plot(fit_info=True)\n",
    "plot.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code hier eingeben\n",
    "# --> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Aufbau einer Kovarianz-Matrix aus einzelnen Unsicherheiten\n",
    "---\n",
    "\n",
    "Behandelt werden: \n",
    "  - der Umgang mit komplexen Unsicherheiten\n",
    "  \n",
    "Eine der besonderen Stärken von _kafe2_ ist die Unterstützung von korrelierten Unsicherheiten.\n",
    "Damit sind Beiträge zur Unsicherheit gemeint, die einige oder alle Werte in gleicher Weise\n",
    "beeinflussen - z.B. weil sie mit dem gleichen, mit einer systematischen Unsicherheit behafteten\n",
    "Messgerät aufgezeichnet wurden.\n",
    "Häufig handelt es sich also um gemeinsame Unsicherheiten von Gruppen von Messwerten.\n",
    "\n",
    "Zur Angabe von Unsicherheiten dient die Funktion:\n",
    "> `add_error**( [axis], err_val, name=None, correlation=0, relative=False)`  \n",
    "  Add an uncertainty source to the data container. Returns an error id which\n",
    "  uniquely identifies the created error source.  \n",
    "  **Parameters**  \n",
    "  • axis (str or int) – 'x'/0 or 'y'/1  \n",
    "  • err_val (float or iterable of float) – pointwise uncertainty/uncertainties for all data points  \n",
    "  • name (str or None) – unique name for this uncertainty source. If None, the name\n",
    "    of the error source will be set to a random alphanumeric string.  \n",
    "  • correlation (float) – correlation coefficient between any two distinct data points  \n",
    "  • relative (bool) – if True, err_val will be interpreted as a relative uncertainty  \n",
    "  **Returns** error name  \n",
    "  **Return type** str  \n",
    "\n",
    "Sie gehört zur Container-Klasse, kann aber auch über eine Fit-Klasse aufgerufen werden.\n",
    "Mit diesem recht einfachen Interface lassen sich sowohl unabhängige Unsicherheiten als auch\n",
    "gemeinsame absolute oder relative Unsicherheiten von Datenpunkten angeben.\n",
    "Die angegebenen Unsicherheiten werden in eine Kovarianzmatrix der Datenpunkte umgewandelt.\n",
    "Bei mehrfachem Aufruf werden die sich ergebenden Kovarianzmatrizen addiert (wie es den Regeln der\n",
    "elementaren Fehlerfortpflanzung entspricht).\n",
    "\n",
    "Ein sehr einfach gehaltenes Beispiel soll das illustrieren.\n",
    "Wir betrachten die Mittelung von vier Werten, die von zwei Gruppen mit unterschiedlichen\n",
    "Messverfahren durchgeführt wurden.\n",
    "Jede der beiden Gruppen gibt zwei Messungen an; in der ersten Gruppe gibt es eine absolute, den\n",
    "beiden Messungen gemeinsame Unsicherheit; die zweite Gruppe gibt eine zwischen ihren beiden\n",
    "Messungen korrelierte relative - also z.B. durch einen Skalierungsfeher verursachte -\n",
    "Unsicherheit an.\n",
    "Den Messungen liegt weiterhin eine gemeinsame (theoretische) Annahme zu Grunde, die zu einer\n",
    "allen Messungen gemeinsamen, absoluten Unsicherheit führt.\n",
    "\n",
    "Bei diesem einfachen Problem nutzen wir die einfachste Datenstruktur von *kafe2*,\n",
    "den _IndexedContainer_, zur Bereitstellung der Daten:  \n",
    "``` python\n",
    "from kafe2 import IndexedContainer\n",
    "idx_data = IndexedContainer([5.3, 5.2, 4.7, 4.8])  \n",
    "```\n",
    "Als Modell wählen wir eine konstante Funktion:\n",
    "``` python\n",
    "# The very simple \"model\":\n",
    "def average (a):\n",
    "  return a\n",
    "```\n",
    "\n",
    "Die Unsicherheiten werden dann folgendermaßen angegeben \n",
    "                (Anm.: Für _IndexedContainer_ entfällt die Angabe des '_axis_'-Parameters!):\n",
    "  1. jeder Messung eigene, unabhängige Unsicherheit  \n",
    "     `err_stat = idx_data.add_error([.2, .2, .2, .2])`\n",
    "  2. die den ersten beiden Werten gemeinsame Unsicherheit  \n",
    "     `err_syst12 = idx_data.add_error([.175, .175, 0., 0.], correlation = 1.)`\n",
    "  3. die den letzten beiden Werten gemeinsame, relative Unsicherheit  \n",
    "     `err_syst34 = idx_data.add_error([0., 0., .05, .05], correlation = 1., relative=True)`\n",
    "  4. die allen Werten gemeinsame Unsicherheit   \n",
    "     `err_syst = idx_data.add_error(0.15, correlation = 1.)`\n",
    "\n",
    "Wir sollten noch passende Namen für die Daten angeben:\n",
    "``` python\n",
    "idx_data.label = 'Testdaten'\n",
    "idx_data.axis_labels = [None, 'Messwert (a.u.)']\n",
    "```\n",
    "\n",
    "Das Ausführen der Anpassung ist mittlerweile ja gut bekannt:\n",
    "``` python\n",
    "# Set up the fit:\n",
    "ifit = Fit(idx_data, average)\n",
    "ifit.model_label = 'Mittelwert'\n",
    "\n",
    "# Perform the fit:\n",
    "ifit.do_fit()\n",
    "```\n",
    "\n",
    "Die Ergebisse erhält man natürlich mit der _report()_-Funktion, ggf. auch als grafische Darstellung:\n",
    "``` python\n",
    "# Report and plot results:\n",
    "ifit.report()\n",
    "p=Plot(ifit)\n",
    "p.plot()\n",
    "p.show()\n",
    "```\n",
    "\n",
    "Die Beschriftung der x-Achse ist noch nicht passend - hier sollten nur die Indizes\n",
    "der Messungen stehen.\n",
    "Mit ein wenig Hilfe von *matplotlib* lässt sich das erreichen.\n",
    "Dazu muss auf das *axis*-Objekts der erzeugten Grafik zugegriffen und die entsprechende Anpassung\n",
    "durchgeführt werden.\n",
    "Dazu folgenden Code nach der Zeile *p.plot()* vor *p.show()* eingefügen:\n",
    "``` python\n",
    "# illustrate some a-posteriory fixes to plot layout by accessing the axis object\n",
    "_ax = p.axes[0]['main']\n",
    "_ax.set_xticks(range(4)) # Integer axis ticks\n",
    "```\n",
    "\n",
    "Wenn ein Problem mehrere Beiträge zur Gesamtunsicherheit enthält, möchte man in der Regel\n",
    "gerne studieren, welchen Einfluss einzelne Komponenten haben.\n",
    "Dazu kann man komfortabel mit den Funktionen *disable_error()* und *enable_error()* arbeiten\n",
    "und entsprechende Anpassungen durchführen:\n",
    "``` python\n",
    "print(\"disabling common sysytematic error\")\n",
    "idx_data.disable_error(err_syst)\n",
    "_ifit = Fit(idx_data, average) \n",
    "_ifit.do_fit()\n",
    "_ifit.report()\n",
    "#      do not forget to switch on again \n",
    "idx_data.enable_error(err_syst)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code hier eingeben\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Anwendung aus der Praxis: Anpassung einer Breit-Wigner-Resonanz \n",
    "---\n",
    "\n",
    "Behandelt werden: \n",
    "  - der Umgang mit komplexen Unsicherheiten\n",
    "  - das Erzeugen einer ansprechenden grafischen Ausgabe\n",
    "  - das Studium des Einflusses einzelner Fehlerkomponenten\n",
    "\n",
    "Typischerweise sind die Unsicherheiten der Messdaten deutlich komplexer als in den bisher\n",
    "behandelten Beispielen.\n",
    "Meist sind Unsicherheiten in Ordinate und Abszisse vorhanden, und zusätzlich zu den unabhängigen\n",
    "Unsicherheiten eines jeden Datenpunktes gibt es allen gemeinsame, korrelierte Unsicherheiten.\n",
    "\n",
    "Mit der Methode *add_error()* bzw. *add_matrix_error()* können Unsicherheiten auf die '*x*'- und\n",
    "'*y*'-Daten spezifiziert werden, entweder in Form von unabhängigen bzw. korrelierten, relativen oder\n",
    "absoluten Unsicherheiten aller oder Gruppen von Messwerten.\n",
    "Oder durch die Angabe der vollständigen Kovarianz- oder Korrelations-Matrix.\n",
    "Alle so spezifizierten Unsicherheiten gehen in die globale Kovarianzmatrix für die Anpassung ein.\n",
    "\n",
    "Als Beispiel betrachten wir Messungen eines Wirkungsquerschnitts als Funktion der Energie in der\n",
    "Nähe einer Resonanz.\n",
    "Es handelt sich dabei um kombinierte Messdaten der vier Experimente am Beschleuniger LEP des\n",
    "CERN, die auf Effekte durch Photon-Abstrahlung korrigiert wurden:\n",
    "Messungen des hadronischen Wirkungsquerschnitts $\\sigma_{e^+e^- \\to {\\rm hadrons}}$ als Funktion\n",
    "der Schwerpunktsenergie $E$.\n",
    "``` python\n",
    "## Data:\n",
    "# Center-of-mass energy E (GeV)\n",
    "E = [ 88.387, 89.437, 90.223, 91.238, 92.059, 93.004, 93.916 ]  \n",
    "E_errors = [ 0.005, 0.0015, 0.005, 0.003, 0.005, 0.0015, 0.005 ]\n",
    "ECor_abs = 0.0017  # correlated absolute errors\n",
    "\n",
    "# hadronic cross section with photonic corrections applied (nb)\n",
    "sig = [6.803, 13.965, 26.113, 41.364, 27.535, 13.362, 7.302 ]  \n",
    "sig_errors = [ 0.036, 0.013, 0.075, 0.010, 0.088, 0.015, 0.045 ]\n",
    "sigCor_rel = 0.0007 \n",
    "```\n",
    "\n",
    "Als Modell verwenden wir eine modifizierte Breit-Wigner-Resonanz mit von der Schwerpunktsenergie\n",
    "abhängiger Breite (\"$s$-dependent width\", mit $s = E_{CM}^2$):\n",
    "``` python\n",
    "## Model:\n",
    "# Breit-Wigner with s-dependent width\n",
    "def BreitWigner(E, s0 = 41.0, M = 91.2, G = 2.5):\n",
    "    s = E*E\n",
    "    Msq = M*M\n",
    "    Gsq = G*G\n",
    "    return s0*s*Gsq/((s-Msq)*(s-Msq)+(s*s*Gsq/Msq))\n",
    "```\n",
    "\n",
    "Der Daten-Container mit den Unsicherheiten wird wie folgt erzeugt:\n",
    "``` python\n",
    "BWdata= XYContainer(ECM, sig)\n",
    "# Add independent errors:\n",
    "error_name_sig = BWdata.add_error(axis='x', name = 'deltaE', err_val = E_errors )   \n",
    "error_name_E = BWdata.add_error(axis='y', name = 'deltaSig', err_val = sig_errors )\n",
    "# Add fully correlated, absolute Energy errors:\n",
    "error_name_ECor = BWdata.add_error(axis='x', name='Ecor',err_val = ECor_abs, correlation = 1.) \n",
    "# Add fully correlated, relative cross section errors:\n",
    "error_name_sigCor = BWdata.add_error(axis='y', name='sigCor', \n",
    "                            err_val = sigCor_rel, correlation = 1., relative=True) \n",
    "```\n",
    "\n",
    "Ob es sich um unabhängige oder korrelierte Unsicherheiten handelt, wird durch den Parameter\n",
    "*correlation* bestimmt;\n",
    "für unabhängige Unsicherheiten ist er Null, für allen Dateneinträgen gemeinsame Unsicherheiten\n",
    "ist er Eins.\n",
    "Werte zwischen 0. und 1. sind ebenfalls zulässig;\n",
    "allerdings wird in der Praxis die Kovarianzmatreix zur Beschreibung der Gesamtunsicherheit meist\n",
    "aus unkorrelierten und vollständig korrelierten Komponenten zusammengesetzt.\n",
    "Die in der Fuktion *add_error* angegebenen Namen erlauben es, später auf die einzelnen\n",
    "Fehlerkomponenten zuzugreifen.\n",
    "\n",
    "Anpassung und Ergebnisausgabe folgen der üblichen Vorgehensweise:\n",
    "``` python\n",
    "BWfit = Fit(BWdata, BreitWigner)\n",
    "BWfit.do_fit()\n",
    "BWfit.report()\n",
    "# Optional: plot the fit results\n",
    "BWplot = Plot(BWfit)\n",
    "BWplot.plot(fit_info=True)\n",
    "BWplot.show()\n",
    "```\n",
    "\n",
    "**Verschönerung der grafischen Ausgabe**  \n",
    "Damit die Art der Daten klar beschrieben ist, sollten noch passende Namen vergeben werden.\n",
    "Die Zeilen unten müssen dazu vor der Erzeugung des *Fit*-Objekts eingefügt werden.\n",
    "``` python\n",
    "BWdata.label = 'QED-corrected hadronic cross-sections'\n",
    "BWdata.axis_labels = ('CM Energy (GeV)', '$\\sigma_h$ (nb)' )\n",
    "```\n",
    "Alternativ können die folgenden Zeilen nach der Erstellung des Fit-Objektes eingefügt werden:\n",
    "``` python\n",
    "BWfit.data_container.label = 'QED-corrected hadronic cross-sections'\n",
    "BWfit.data_container.axis_labels = ('CM Energy (GeV)', r'$\\sigma_h$ (nb)')\n",
    "```\n",
    "\n",
    "Es sollte auch noch ein passender Name für das Modell in der Legende für die grafische Ausgabe\n",
    "gesetzt werden.\n",
    "Dazu wird die Zeile unten nach der Erzeugung des *Fit*-Objekts eingesetzt:\n",
    "``` python\n",
    "BWfit.model_label = 'Beit-Wigner with s-dependent width'\n",
    "```\n",
    "\n",
    "Falls ein schön gesetzter Ausdruck für die Modellfunktion gewünscht wird, können LaTeX-Namen für\n",
    "das Modell, die Parameter und die Modellfunkton gesetzt werden:\n",
    "``` python\n",
    "# Set LaTeX names for printout in info-box:\n",
    "BWfit.assign_parameter_latex_names(E='E', s0=r'{\\sigma^0}', M=r'{M_Z}', G=r'{\\Gamma_Z}')\n",
    "BWfit.assign_model_function_latex_name(r'\\sigma^{\\rm ew}_{e^+e^-\\to{\\rm hadrons}}')\n",
    "BWfit.assign_model_function_latex_expression(\n",
    "               r'{s0}\\frac{{ {E}^2{G}^2}}{{({E}^2-{M}^2)^2+({E}^4{G}^2/{M}^2)}}')\n",
    "```\n",
    "\n",
    "Anmerkung: Die Verdopplung der Klammern \"{\" und \"}\" ist notwendig, weil sie in *kafe2*,\n",
    "ähnlich wie in der Python *format*-Funktion, auch zur Übergabe von Parametern genutzt werden.\n",
    "\n",
    "Wir haben bereits gesehen, wie man die Bezeichnung für das Band für die Anzeige der\n",
    "Modellunsicherheit modifizieren kann:\n",
    "``` python\n",
    "BWplot.customize('model_error_band', 'label', [r'$\\pm 1\\sigma$'])\n",
    "```\n",
    "\n",
    "In diesem Beispiel ist allerdings die Modellunsicherheit extrem klein (weit unter 0.1%) und daher\n",
    "in der Grafik nicht sichtbar.\n",
    "Unterdrücken kann man die Ausgabe in der Legende mit folgender Angabe:\n",
    "``` python\n",
    "BWplot.customize('model_error_band', 'label', [None])\n",
    "```\n",
    "\n",
    "Manchmal wird das Unsicherheitsband von der Line überdeckt; in solchen Fällen solle eine\n",
    "gestrichelte oder gepunktete Linie für das Modell verwendet werden:\n",
    "``` python\n",
    "BWplot.customize('model_line', 'linestyle', [':'])\n",
    "```\n",
    "\n",
    "Nun können noch die Ränder des Plot-Bereiches angepasst werden.\n",
    "Dies gelingt über die Properties *x_range* und *y_range* der Plot-Klasse:\n",
    "``` python\n",
    "BWplot.x_range = (88, 94)\n",
    "BWplot.y_range = (0, 45)\n",
    "```\n",
    "\n",
    "Da es sich um eine nichtlineare Anpassung handelt, sollten noch Profile-Likelihood \n",
    "und Konfidenz-Konturen angezeigt werden.\n",
    "Die folgende Zeile muss dazu vor *BWplot.show()* eingefügt werden:\n",
    "``` python\n",
    "ContoursProfiler(BWfit).plot_profiles_contours_matrix(show_grid_for='contours')\n",
    "``` \n",
    "!!! Geduld: die Berechnung der Konturen ist rechenaufwändig und dauert eine gewisse Zeit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' the data for the Breit-Wigner example'''\n",
    "# Center-of-mass energies E (GeV):\n",
    "ECM = [ 88.387, 89.437, 90.223, 91.238, 92.059, 93.004, 93.916 ]  \n",
    "E_errors = [ 0.005, 0.0015, 0.005, 0.003, 0.005, 0.0015, 0.005 ]\n",
    "ECor_abs = 0.0017  # Correlated absolute errors.\n",
    "\n",
    "# Hadronic cross sections with photonic corrections applied (nb):\n",
    "sig = [6.803, 13.965, 26.113, 41.364, 27.535, 13.362, 7.302 ]  \n",
    "sig_errors = [ 0.036, 0.013, 0.075, 0.010, 0.088, 0.015, 0.045 ]\n",
    "sigCor_rel = 0.0007 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obigen Code hier eingeben \n",
    "# --> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Studium des Einflusses einzelner Fehlerkomponenten**  \n",
    "Um den Einfluss einzelner Fehlerkomponenten auf das Ergebnis zu untersuchen, kann man einzelne\n",
    "Quellen von Unsicherheiten mit der Methode *disable_error()* abschalten und eine neue Anpassung\n",
    "ausführen, hier gezeigt für die korrelierte Unsicherheit der Schwerpunktsenergien:\n",
    "``` python\n",
    "print('!!!  disabling error component ', error_name_ECor)\n",
    "BWfit.disable_error(error_name_ECor)\n",
    "BWfit.do_fit()\n",
    "BWfit.report(show_data=False, show_model=False)\n",
    "\n",
    "# do not forget to switch on again !\n",
    "print('!!!  re-enabling error component ', error_name_ECor)\n",
    "BWfit.enable_error(error_name_ECor)\n",
    "\n",
    "#### fallback option with new fit object\n",
    "#print('!!!  disabling error component ', error_name_ECor)\n",
    "#BWdata.disable_error(error_name_ECor)\n",
    "#_fit = Fit(BWdata, BreitWigner)\n",
    "#_fit.do_fit()\n",
    "#_fit.report(show_data=False, show_model=False)\n",
    "#BWdata.enable_error(error_name_ECor)\n",
    "```\n",
    "\n",
    "Das Ergebnis ist fast identisch zum vorherigen, lediglich die Unsicherheit der Masse ist nun\n",
    "kleiner.\n",
    "Dies war auch so zu erwarten, denn eine korrelierte Änderung aller Energien sollte die Breite\n",
    "oder Höhe der Resonanz nicht beeinflussen.\n",
    "\n",
    "Mit der Methode *enable_error(error_name_ECor)* wird die Fehlerquelle wieder aktiviert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier ausprobieren\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6. Anpassung an Histogramm-Daten\n",
    "***\n",
    "\n",
    "Im Prinzip lässt sich auch die Anpassung einer Verteilungsdichte an eine Häufigkeitsverteilung\n",
    "als Funktionsanpassung auffassen.\n",
    "Allerdings gibt es einige Besonderheiten, die berücksichtigt werden müssen:\n",
    "\n",
    "- Der dem Wert einer Verteilungsdichte (PDF=Particle Density Function) entsprechende\n",
    "  Funktionswert für ein Bin entspricht dem Integral der PDF über das Bin\n",
    "- Die Unsicherheit eines Bin-Eintrages ergibt sich aus der Poisson-Verteilung, die nur\n",
    "  bei sehr großen Zahlen an Einträgen pro Bin durch eine Gauß-Verteilung angenähert werden kann.\n",
    "   \n",
    "*kafe2* bietet daher eine spezielle Methode zur Anpassung einer Vereilungsdichte an Histogramme,\n",
    "die Klassen *HistContainer* zur Abspeicherung der Histogrammdaten und *HistFit* zur\n",
    "Durchführung der Anpassungen:\n",
    "``` python\n",
    "from kafe2 import HistContainer, HistFit\n",
    "```\n",
    "\n",
    "Als Kostenfunktion zur Bewertung der Übereinstimmung der angepassten PDF mit den Bin-Einträgen\n",
    "in der Häufigkeitsverteilung wird das Doppelte des negativen Logarithmus der Poisson-Likelihood\n",
    "verwendet, andere Optionen sind konfigurierbar. \n",
    "\n",
    "In diesem einfachen Beispiel verwenden wir die Häufigkeitsverteilung von Gauß-verteilten\n",
    "Zufallszahlen, an die eine Gaußverteilung angepasst wird.\n",
    "``` python\n",
    "def normal_distribution_pdf(x, mu, sigma):\n",
    "  return np.exp(-0.5 * ((x - mu) / sigma) ** 2) / np.sqrt(2.0 * np.pi * sigma** 2)\n",
    "```\n",
    "\n",
    "Die Daten werden zufällig aus der Standardnormalverteilung erzeugt:\n",
    "``` python\n",
    "# create a random dataset of 100 random values, \n",
    "#  following a standard normal distribution with mu=0 and sigma=1\n",
    "data = np.random.normal(loc=0, scale=1, size=100)\n",
    "```\n",
    "\n",
    "Der Datencontainer und das Fit-Objekt werden analog zu den früheren Beispielen erstellt:\n",
    "``` python\n",
    "# Create a histogram from the dataset by specifying the bin range and the number of bins.\n",
    "# Alternatively the bin edges can be set.\n",
    "histogram = HistContainer(n_bins=10, bin_range=(-5, 5), fill_data=data)\n",
    "\n",
    "# create the Fit object by specifying a density function\n",
    "fit = HistFit(data=histogram, model_function=normal_distribution_pdf)\n",
    "```\n",
    "\n",
    "Durchführung der Anpassung und Ausgabe der Ergebnisse unterscheiden sich nicht von der\n",
    "Vorgehensweise bei den früheren Beispielen:\n",
    "``` python\n",
    "# do the fit\n",
    "fit.do_fit()\n",
    "\n",
    "# Optional: print a report to the terminal\n",
    "fit.report()\n",
    "\n",
    "# Optional: create a plot and show it\n",
    "phist = Plot(fit)\n",
    "phist.plot()\n",
    "phist.show()\n",
    "```\n",
    "\n",
    "An dieser Stelle sollten wir noch einmal die Möglichkeiten zur Anpassung der grafischen Ausgabe\n",
    "anschauen.\n",
    "Der Plot-Adapter für Histogramme kennt als *plot_type* die Werte *data*, *model* und\n",
    "*model_density*.\n",
    "Über den Aufruf von `print(phist.get_keywords(<plot_type>))` können die möglichen Parameter zur\n",
    "Einstellung ausgebeben werden.\n",
    "Hier ein Vorschlag für Code zur Anpassung der Grafikausgabe, der vor dem Befehl *phist.plot()*\n",
    "stehen muss:\n",
    "``` python\n",
    "## reprise: plot customization\n",
    "#    data\n",
    "phist.customize('data', 'label', [\"random Gaussian data\"] ) \n",
    "phist.customize('data', 'marker', ['o'])\n",
    "phist.customize('data', 'markersize', [5])\n",
    "phist.customize('data', 'color', ['blue']) \n",
    "phist.customize('data', 'ecolor', ['blue']) \n",
    "#    model\n",
    "phist.customize('model_density', 'label', [\"Gaussian PDF\"])\n",
    "phist.customize('model_density', 'color', [\"black\"])\n",
    "phist.customize('model', 'label', [\"entries per bin\"])\n",
    "phist.customize('model', 'facecolor', [\"lightgrey\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier ausprobieren \n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7. Likelihood-Anpassungen\n",
    "***\n",
    "\n",
    "Wenn nur wenige Messungen vorhanden sind, ist es nicht möglich, eine sinnvolle\n",
    "Häufigkeitsverteilung zu erhalten, denn eine grobe Einteilung in Bins würde die Messungen\n",
    "verfälschen, während eine zu feine Einteilung zu Bins mit sehr wenigen oder gar null Einträgen\n",
    "führen würde.\n",
    "Das oben schon angewendete Verfahren zur Anpassung einer Verteilungsdichte an eine\n",
    "Häufigkeitsverteilung ist dann nicht anwendbar.\n",
    "In solchen Fällen verwendet man eine direkte Anpassung mit Hilfe des\n",
    "Maximum-Likelihood-Verfahrens and die ungebinnten Daten.\n",
    "Auch dieses Verfahren ist in *kafe2* implementiert.\n",
    "Dazu müssen nur die passenden Klassen importiert werden:\n",
    "``` python\n",
    "from kafe2.fit import UnbinnedContainer, UnbinnedFit\n",
    "```\n",
    "\n",
    "In diesem Beispiel verwenden wir zur Illustration 160 einzelne Messungen der Lebensdauer von in\n",
    "einem Detektor gestoppten Myonen aus der kosmischen Strahlung.\n",
    "Die Häufigkeitsverteilung ist eine Exponentialverteilung über flachem Untergrund:\n",
    "``` python\n",
    "def pdf(t, tau=2.2, fbg=0.1, a=1., b=9.75):\n",
    "  \"\"\"\n",
    "  Probability density function for the decay time of a myon. \n",
    "  The pdf is normalized to an integral of one for the interval (a, b).\n",
    "  :param t: decay time\n",
    "  :param fbg: background\n",
    "  :param tau: expected mean of the decay time\n",
    "  :param a: the minimum decay time which can be measured\n",
    "  :param b: the maximum decay time which can be measured\n",
    "  :return: probability for decay time x\n",
    "  \"\"\"\n",
    "  pdf1 = np.exp(-t / tau) / tau / (np.exp(-a / tau) - np.exp(-b / tau))\n",
    "  pdf2 = 1. / (b - a)\n",
    "  return (1 - fbg) * pdf1 + fbg * pdf2\n",
    "```\n",
    "\n",
    "Zu beachten ist, dass die Häufigkeitsverteilung für alle möglichen Parameterwerte auf Eins\n",
    "normiert sein muss!\n",
    "\n",
    "Zur Vorgehensweise bei der Anpassung gibt es nur eine kleine Besonderheit: \n",
    "der Untergrundanteil ist auf Grund der geringen Anzahl an Beobachtungen mit einer großen\n",
    "Unsicherheit behaftet und kann daher bei der Variation im Verlauf des Anpassungsalgorithmus sogar\n",
    "negativ werden.\n",
    "Um diesen \"unphysikalischen\" Bereich des Parameters zu vermeiden, gibt es die Option\n",
    "`fit.limit_parameter(<name>, lower=<min>, upper=<max> )`.\n",
    "\n",
    "Alle weiteren Schritte im folgenden Beispielcode sind bereits bekannt:\n",
    "``` python\n",
    "data = UnbinnedContainer(dT) # create the kafe data object\n",
    "data.label = 'lifetime measurements'\n",
    "data.axis_labels = ('Myon Life Time ' r'$\\tau$' ' (µs)','Density' )\n",
    "\n",
    "# create the fit object and set the pdf for the fit\n",
    "LLfit = UnbinnedFit(data=data, model_function = pdf)\n",
    "\n",
    "# assign latex names for model and parameters for nicer display\n",
    "LLfit.model_label = 'Exponential decay + flat background'\n",
    "LLfit.assign_parameter_latex_names(t='t', tau=r'\\tau', fbg='f', a='a', b='b')\n",
    "LLfit.assign_model_function_latex_expression(\"\\\\frac{{ (1-{fbg}) \\, e^{{-{0}/{tau}}}}}\"\n",
    "    \"{{{tau}(e^{{-{a}/{tau}}}-e^{{-{b}/{tau}}})}} + \\\\frac{{ {fbg} }} {{ {b}-{a} }}\")\n",
    "\n",
    "# Fix the parameters a and b ...\n",
    "a = 1.0\n",
    "b = 11.5\n",
    "LLfit.fix_parameter(\"a\", a)\n",
    "LLfit.fix_parameter(\"b\", b)\n",
    "# ... and limit parameter fbg\n",
    "LLfit.limit_parameter(\"fbg\", lower=0., upper=1.)\n",
    "\n",
    "LLfit.do_fit()  # perform the fit\n",
    "LLfit.report(asymmetric_parameter_errors=True)\n",
    "\n",
    "pLL = Plot(LLfit)  # create a plot object\n",
    "pLL.x_range = [a, b]\n",
    "pLL.plot(fit_info=True, asymmetric_parameter_errors=True)  # plot the data and the fit\n",
    "#pLL.axes[0]['main'].set_xlabel('Life time '+r'$\\tau$'+' (µs)', size='large')  # overwrite the x-axis label\n",
    "\n",
    "cpfLL = ContoursProfiler(LLfit, profile_subtract_min=False)  # Optional: create a contours profile\n",
    "cpfLL.plot_profiles_contours_matrix(parameters=['tau', 'fbg'])  # Optional: plot the contour matrix for tau and fbg\n",
    "\n",
    "cpfLL.show()  # show the plot(s)\n",
    "```\n",
    "\n",
    "Interessant ist die spezielle Form der grafischen Darstellung der Daten, bei der in diesem Fall\n",
    "jeder Messwert durch einen Strich dargestellt wird.\n",
    "Die Dichte der Striche pro Längeneinheit entspricht der Verteilungsdichte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' the data for the myon life time example'''\n",
    "# real data from measurement with a Water Cherenkov detector (\"Kamiokanne\")\n",
    "dT = [7.42, 3.773, 5.968, 4.924,  1.468,  4.664,  1.745,  2.144,  3.836,  3.132,\n",
    "  1.568,  2.352,  2.132,  9.381,  1.484,  1.181,  5.004,  3.06,   4.582,  2.076,\n",
    "  1.88,   1.337,  3.092,  2.265,  1.208,  2.753,  4.457,  3.499,  8.192,  5.101,\n",
    "  1.572,  5.152,  4.181,  3.52,   1.344, 10.29,   1.152,  2.348,  2.228,  2.172,\n",
    "  7.448,  1.108,  4.344,  2.042,  5.088,  1.02,   1.051,  1.987,  1.935,  3.773,\n",
    "  4.092,  1.628,  1.688,  4.502,  4.687,  6.755,  2.56,   1.208,  2.649,  1.012,\n",
    "  1.73,   2.164,  1.728,  4.646,  2.916,  1.101,  2.54,   1.02,   1.176,  4.716,\n",
    "  9.671,  1.692,  9.292, 10.72,   2.164,  2.084,  2.616,  1.584,  5.236,  3.663,\n",
    "  3.624,  1.051,  1.544,  1.496,  1.883,  1.92,   5.968,  5.89,   2.896,  2.76,\n",
    "  1.475,  2.644,  3.6,    5.324,  8.361,  3.052,  7.703,  3.83,   1.444,  1.343,\n",
    "  4.736,  8.7,    6.192,  5.796,  1.4,    3.392,  7.808,  6.344,  1.884,  2.332, \n",
    "  1.76,   4.344,  2.988,  7.44,   5.804,  9.5,    9.904,  3.196,  3.012,  6.056, \n",
    "  6.328,  9.064,  3.068,  9.352,  1.936,  1.08,   1.984,  1.792,  9.384, 10.15,   \n",
    "  4.756,  1.52,   3.912,  1.712, 10.57,   5.304,  2.968,  9.632,  7.116, 1.212,\n",
    "  8.532,  3.000,  4.792,  2.512,  1.352,  2.168,  4.344,  1.316,  1.468, 1.152,\n",
    "  6.024,  3.272,  4.96,  10.16,   2.14,   2.856, 10.01,   1.232, 2.668, 9.176 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood-Anpassung hier ausprobieren\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 8. Multi-Fits:\n",
    "### simultane Anpassung von Modellfunktionen an verschiedene Datensätze\n",
    "***\n",
    "\n",
    "Sehr oft sind die Modelle zu komplex, um alle Parameter in einer Anpassung an ein\n",
    "einziges Modell zu bestimmen.\n",
    "Modellparameter sind oftmals das Ergebniss mehrerer Modellanpassungen, oder der\n",
    "selbe Parameter kommt in verschiedenen Messreihen vor.\n",
    "\n",
    "Für solche Fälle bietet *kafe2* die Möglichkeit, mehrere Anpassungen unterschiedlicher Modelle\n",
    "mit gemeinsamen Parametern an verschiedene Datensätze durchzuführen.\n",
    "\n",
    "Dazu muss zusätzlich das Paket *MultiFit* importiert werden:\n",
    "``` python\n",
    "from kafe2 import MultiFit\n",
    "```\n",
    "\n",
    "Wir betrachten als einfaches Beispiel die Bestimmung eines ohmschen Widerstands bei\n",
    "Zimmertemperatur, der sich bei höherem Stromfluss erwärmt und so seinen Widerstand gemäß seines\n",
    "Temperaturkoeffizienten ändert.\n",
    "Zusätzlich zum Strom durch den Widerstand wird daher noch die Temperatur für jeden vorgegebenen\n",
    "Spannungswert gemessen.\n",
    "Es müssen also Triplets von Messwerten ausgewertet werden.\n",
    "\n",
    "Die Temperaturabhängigkeit wird empirisch durch ein einfaches quadratisches Modell beschreiben:\n",
    "``` python\n",
    "# empirical model for T(U): a parabola\n",
    "def empirical_T_U_model(U, p2=1.0, p1=1.0, p0=0.0):\n",
    "    # use quadratic model as empirical temperature dependence T(U)\n",
    "    return p2 * U**2 + p1 * U + p0\n",
    "```\n",
    "\n",
    "Der Widerstand als Funktion der Temperatur ist durch den Temperaturkoeffizienten $\\alpha$ gegeben\n",
    "und wird folgendermaßen modelliert:\n",
    "``` python\n",
    "# model of current-voltage dependence I(U) for a heating resistor\n",
    "def I_U_model(U, R0=1., alph=0.004, p2=1.0, p1=1.0, p0=0.0):\n",
    "    # use quadratic model as empirical temperature dependence T(U)\n",
    "    t_ref = 0.\n",
    "    _delta_t = empirical_T_U_model(U, p2, p1, p0) - t_ref\n",
    "    # plug the temperature into the model\n",
    "    return U / (R0 * (1.0 + _delta_t * alph))\n",
    "```\n",
    "Das Modell für den Widerstand enthält also in diesm Fall das erste Modell für die Abhängigkeit\n",
    "der Temperatur von dem durch die angelegte Spannung bestimmten Strom.\n",
    "\n",
    "Hier die Daten für dieses Beispiel:\n",
    "``` python\n",
    "# the data \n",
    "U = [ 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5,   \n",
    "      6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0 ] \n",
    "I = [ 0.5,  0.89, 1.41, 1.67, 2.3,  2.59, 2.77, 3.57, 3.94,  4.24, 4.73,\n",
    "      4.87, 5.35, 5.74, 5.77, 6.17, 6.32, 6.83, 6.87, 7.17 ]\n",
    "T = [ 20.35, 20.65, 22.25, 23.65, 26.25, 27.85, 29.85, 34.25, 37.75, 41.95,\n",
    "     44.85, 50.05, 54.25, 60.55, 65.05, 69.95, 76.85, 81.55, 85.45, 94.75 ]\n",
    "sigU, sigI, sigT = 0.2, 0.1, 0.5 # uncertainties\n",
    "```\n",
    "\n",
    "Die Fit-Prozedur unterscheidet sich kaum von der bisher vorgestellten Vorgehensweise. \n",
    "Zunächst werden die Daten-Container und Anpassungen für die beiden Modelle definiert:\n",
    "``` python\n",
    "# Step 1: construct the singular data containers and fit objects\n",
    "TU_data = XYContainer(U,T)\n",
    "TU_data.label = 'Temperaturen'\n",
    "TU_data.axis_labels = ['Spannung (V)','Temperatur (°C)']\n",
    "fit_1 = Fit(TU_data, model_function=empirical_T_U_model)\n",
    "fit_1.model_label = 'Parametrisierung'\n",
    "\n",
    "IU_data = XYContainer(U,I)\n",
    "IU_data.label = 'Ströme'\n",
    "IU_data.axis_labels = ['Spannung (V)','Strom (A)']\n",
    "fit_2 = Fit(IU_data, model_function=I_U_model)\n",
    "fit_2.model_label = 'Temperaturabhängiger Leitwert'\n",
    "\n",
    "```\n",
    "\n",
    "Dann werden beide Anpassungen zu einem *MultiFit* zusammengefasst.\n",
    "``` python\n",
    "# Step 2: construct a MultiFit object\n",
    "multi_fit = MultiFit(fit_list=[fit_1, fit_2], minimizer='iminuit')\n",
    "```\n",
    "Erst jetzt werden die Unsicherheiten - dieses Mal zu den\n",
    "Fit-Objekten, hinzugefügt. Dadurch können auch die in beiden\n",
    "Datensätzen gemeinsamen Unsicherheiten auf der x-Achse berücksichtigt \n",
    "werden. \n",
    "``` python\n",
    "# Step 3: Add errors (to the fit object in this case)\n",
    "multi_fit.add_error(sigT, 0, axis='y')  # declare errors on T\n",
    "multi_fit.add_error(sigI, 1, axis='y')  # declare errors on I\n",
    "multi_fit.add_error(sigU, 'all', axis='x') # shared error on x axis\n",
    "```\n",
    "\n",
    "Es folgt noch die Definition aussagekräftiger Namen für die Ausgabe:\n",
    "``` python\n",
    "# (Optional): assign names for models and parameters\n",
    "multi_fit.assign_parameter_latex_names(\n",
    "    U='U', p2='p_2', p1='p_1', p0='p_0', R0='R_0', alph=r'\\alpha_\\mathrm{T}')\n",
    "multi_fit.assign_model_function_expression('{p2}*{U}^2 + {p1}*{U} + {p0}', fit_index=0)\n",
    "multi_fit.assign_model_function_latex_expression(r'{p2}\\,{U}^2 + {p1}\\,{U} + {p0}', fit_index=0)\n",
    "multi_fit.assign_model_function_expression('{U} / ({R0} * (1 + ({p2}*{U}^2 + {p1}*{U} + {p0}) * {alph}))', fit_index=1)\n",
    "multi_fit.assign_model_function_latex_expression(r'\\frac{{{U}}}{{{R0} \\cdot (1 + ({p2}{U}^2 + {p1}{U} + {p0}) \\cdot {alph})}}', fit_index=1)\n",
    "```\n",
    "\n",
    "Der Rest läuft dann genau so wie schon oft gezeigt:\n",
    "``` python\n",
    "# Step 4: do the fit\n",
    "multi_fit.do_fit()\n",
    "\n",
    "# (Optional): print the results\n",
    "multi_fit.report()\n",
    "\n",
    "# (Optional): plot the results\n",
    "plot = Plot(multi_fit, separate_figures=True)\n",
    "plot.customize('data', 'marker', ['.','.'])\n",
    "plot.customize('data', 'markersize', [6,6])\n",
    "\n",
    "plot.plot()\n",
    "\n",
    "plot.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenen Code hier eingeben\n",
    "# -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
