{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Jupyter notebook tutorial:\n",
    "#    Fitting models to data with *kafe2*\n",
    "\n",
    "                                                Johannes Gäßler, October 2022\n",
    "                                                Günter Quast, April 2020\n",
    "                                                Cedric Verstege, August 2020\n",
    "---\n",
    "## Jupyter Notebook Fundamentals\n",
    "\n",
    "This file of type `.ipynb` contains a tutorial as a `Jupyter notebook`.\n",
    "`Jupyter` provides a browser interface with a (simple) development environment for *Python* code\n",
    "and explanatory texts in intuitive *Markdown* format.\n",
    "The input of formulas in *LaTeX* format is also supported.\n",
    "\n",
    "A summary of the most important commands for using *Jupyter* as a working environment can be\n",
    "found in the notebook\n",
    "[*JupyterCheatsheet.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/JupyterCheatsheet.ipynb)\n",
    "(German).\n",
    "Basics for statistical data analysis can be found in the notebooks\n",
    "[*IntroStatistik.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/IntroStatistik.ipynb)\n",
    "(German) and\n",
    "[*Fehlerrechnung.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/Fehlerrechnung.ipynb) (German).\n",
    "\n",
    "In *Jupyter*, code and text are entered into individual cells.\n",
    "Active cells are indicated by a blue bar in the margin.\n",
    "They can be in two states: in edit mode the input field is white, in command mode it is grayed out.\n",
    "Clicking in the border area selects the command mode, clicking in the text field of a code cell\n",
    "switches to edit mode.\n",
    "The `esc` key can also be used to leave the edit mode.\n",
    "\n",
    "Pressing `a` in command mode creates a new empty cell above the active cell, `b` creates one below.\n",
    "Entering `dd` deletes the corresponding cell.\n",
    "\n",
    "Cells can be either of the type `Markdown` or `Code`.\n",
    "Entering `m` in command mode sets the type Markdown, entering `y` selects the type Code.\n",
    "\n",
    "The cell content is processed - i.e. text is formatted or code is executed - by entering `shift+return`,\n",
    "or `alt+return` if a new, empty cell should also be created.\n",
    "\n",
    "The settings mentioned here as well as the insertion, deletion or execution of cells can also be\n",
    "executed via the pull-down menu at the top.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview: *kafe*2\n",
    "***\n",
    "\n",
    "\n",
    "*kafe2* is a successor to the *kafe* package developed since 2012 for the fitting of model\n",
    "functions to data.\n",
    "\n",
    "It supports different data types like simple indexed data,\n",
    "two-dimensional data points (one quantity *x* and one dependent quantity *y*),\n",
    "or one-dimensional data that can be binned as a histogram.\n",
    "Uncertainties of both dependent and independent quantities and, if applicable,\n",
    "their correlations are supported.\n",
    "For this purpose, the global covariance matrix is created from different types\n",
    "of specified uncertainties and taken into account in the fitting.\n",
    "Compared to many other fitting tools, this possibility is a unique feature of *kafe(2)*.\n",
    "\n",
    "Simultaneous fitting of several models is also supported.\n",
    "Each with its own and additionally all or several common parameters\n",
    "to different data sets (\"multi fit\").\n",
    "\n",
    "To minimize the distance between data and model function(s) numerical methods are applied,\n",
    "which are derived from the open source,\n",
    "*Python*-based software environment *SciPy* or the *MINUIT* package developed at CERN.\n",
    "The respective minimized distance metric (or the \"cost function\") is equal to the negative\n",
    "natural logarithm of the likelihood function for the data and the given model multiplied by a\n",
    "factor of two ($-2\\,\\ln{\\cal L}$).\n",
    "For Gaussian uncertainties of the data points, this corresponds to the method\n",
    "of least squares (also called \"$\\chi^2$ method\").\n",
    "Other cost functions based on the method of maximum likelihood are also available for the fitting of probability densities to histograms or indexed data.\n",
    "\n",
    "To determine the confidence intervals of idividual model parameters as well as\n",
    "two-dimensional confidence contours for pairs of parameters *kafe2* makes use of the\n",
    "profile likelihood method.\n",
    "\n",
    "*kafe2* contains a stand-alone application *kafe2go*, which allows fittings without the creation of own code;\n",
    "data, model function and options are specified in a *YAML* configuration file.\n",
    "A fit can then be performed from the command line by calling:\n",
    "\n",
    "`kafe2go <name>.yaml`\n",
    "\n",
    "Alternatively *kafe2* can be used as part of a *Python* program.\n",
    "For the most common use cases *kafe2* has built-in pipelines.\n",
    "The procedure to use these pipelines is to first call a method that\n",
    "does the fit, and to then call another method that produces a plot.\n",
    "\n",
    "**The following examples show the actual procedure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General settings and useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function  # python2-compatibility\n",
    "import sys, os\n",
    "\n",
    "# Lines with % or %% at the beginning are so-called \"magic commands\",\n",
    "# that specify the cell type or options for displaying graphics.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Presets for *kafe2*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import kafe2\n",
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# set better default figure size for kafe2\n",
    "# plt.rcParams['figure.figsize']=[12., 5.]\n",
    "#        !!!  must be done after importing kafe2 (will else be overwritten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## 1. Simple example for fitting with `kafe2`\n",
    "***\n",
    "\n",
    "The following code illustrates the fitting of functions with the *kafe2* fitting tool:\n",
    "\n",
    "``` python\n",
    "# Define or read in the data for your fit:\n",
    "x_data = [1.0, 2.0, 3.0, 4.0]\n",
    "y_data = [2.3, 4.2, 7.5, 9.4]\n",
    "# x_data and y_data are combined depending on their order.\n",
    "# The above translates to the points (1.0, 2.3), (2.0, 4.2), (3.0, 7.5), and (4.0, 9.4).\n",
    "\n",
    "# Important: Specify uncertainties for the data!\n",
    "x_error = 0.1\n",
    "y_error = 0.4\n",
    "\n",
    "# Pass the information to kafe2:\n",
    "kafe2.xy_fit(\"line\", x_data, y_data, x_error=x_error, y_error=y_error)\n",
    "# The string \"line\" gets mapped to a first degree polynomial for the model function.\n",
    "\n",
    "# Call another function to create a plot:\n",
    "kafe2.plot(\n",
    "    x_label=\"x\",  # x axis label\n",
    "    y_label=\"y\",  # y axis label\n",
    "    data_label=\"Data\",  # label of data in legend\n",
    ")\n",
    "```\n",
    "\n",
    "Paste the code into the empty cell below and execute it by pressing `shift+return`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# simple example: straight line adjustment with kafe2\n",
    "# -> insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be seeing two plots.\n",
    "The first one is a regular plot that shows the data, the model, and the fit results.\n",
    "The second one is a plot showing the so-called profile likelihood of the fit parameters; let's ignore this plot for now.\n",
    "\n",
    "The plots and the fit results have also been saved to disk.\n",
    "Try opening the subdirectory `results`.\n",
    "There should be two png files for the plots, one txt file that contains a human-readable report of the fit results, and one machine-readable yml file that contains the fit results in the YAML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Correlated uncertainties\n",
    "***\n",
    "\n",
    "To illustrate the possibilities for dealing with uncertainties, we add a further correlated\n",
    "uncertainty of the dependent quantities *y*:\n",
    "``` python\n",
    "kafe2.xy_fit(\"line\", x_data, y_data, x_error=x_error, y_error=y_error, y_error_cor=0.3)\n",
    "```\n",
    "Notice how there is an additional keyword argument `y_error_cor` that specified the correlated *y* uncertainty.\n",
    "Extend the call to `kafe2.xy_fit` in the above code to also specify a correlated *y* uncertainty.\n",
    "Now repeat the fit.\n",
    "\n",
    "As expected, such an uncertainty common to all data points does not affect the gradient of the\n",
    "straight line, but only the parameter *b*, whose uncertainty now becomes greater - corresponding\n",
    "to the square root of the sum of the squared uncertainties of $\\pm 0.58$ from the original fit and the\n",
    "additional correlated uncertainty of $\\pm 0.40$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copy and extend code from the previous example\n",
    "# ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 How to find keyword arguments for *kafe2* functions\n",
    "***\n",
    "\n",
    "Obviously it is possible to define correlated uncertainties for the `kafe2.xy_fit` function.\n",
    "But in order to use this functionality you would first need to know that the keyword argument `y_error_cor` exists.\n",
    "So how could you figure this out?\n",
    "The first possiblity is to consult the *kafe2* documentation which describes the signatures and arguments of all user-facing functions.\n",
    "But you can also access this information directly from a Jupyter notebook by accessing the built-in help system.\n",
    "For example, to get information about the `kafe2.plot` method simply copy the following to the cell below:\n",
    "``` python\n",
    "? kafe2.plot\n",
    "```\n",
    "The output from the Jupyter help has several parts.\n",
    "First it prints the signature of the function and what the default values for the arguments are (for `kafe2.plot`the value `None` means that the value should be inferred from the fit).\n",
    "Then it prints the function docstring which describes the meaning and expected data types of the various arguments.\n",
    "Finally it prints the location of the function source.\n",
    "\n",
    "Unfortunately the Jupyter help system does not format function docstrings in the same way that the documentation does so they may be a little hard to read but it's a very quick way of figuring out how a function should be used.\n",
    "Integrated development environments and advanced text editors (PyCharm, VSCode, Emacs, etc.) usually also have a built-in help system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just copy \"? kafe2.plot\" (without quotation marks) here to get help.\n",
    "# ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Comparison of Two Different Models\n",
    "***\n",
    "\n",
    "On sufficiently small scales every function can be approximated by a line due to Taylor expansion.\n",
    "However, for many problems this approximation is insufficient.\n",
    "The following example shows the fitting of a linear and an exponential model to the same data.\n",
    "\n",
    "To define a model function for *kafe2* simply write it as a *Python* function.\n",
    "Important: The first argument of the model function is interpreted as the\n",
    "independent variable of the fit. It is not being changed during the fit and it's the\n",
    "quantity represented by the *x* axis of the fit.\n",
    "\n",
    "\n",
    "Definition of two model functions:\n",
    "``` python\n",
    "# Our first model is a simple linear function:\n",
    "def linear_model(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "# Our second model is a simple exponential function.\n",
    "# The kwargs in the function header specify parameter defaults.\n",
    "def exponential_model(x, A_0=1., x_0=5.):\n",
    "    return A_0 * np.exp(x/x_0)\n",
    "```\n",
    "\n",
    "Definition of the data as a *kafe2* `XYContainer`:\n",
    "``` python\n",
    "# The data for this exercise:\n",
    "x_data_2 = [19.8, 3.0, 5.1, 16.1, 8.2, 11.7, 6.2, 10.1]\n",
    "y_data_2 = [23.2, 3.2, 4.5, 19.9, 7.1, 12.5, 4.5, 7.2]\n",
    "x_error_2 = 0.3\n",
    "y_error_rel_2 = 0.15\n",
    "```\n",
    "The function `kafe2.xy_fit` needs to be called twice to do two fits:\n",
    "``` python\n",
    "results_linear = kafe2.xy_fit(\n",
    "    linear_model, x_data_2, y_data_2,\n",
    "    x_error=x_error_2, y_error_rel=y_error_rel_2)\n",
    "results_exponential = kafe2.xy_fit(\n",
    "    exponential_model, x_data_2, y_data_2,\n",
    "    x_error=x_error_2, y_error_rel=y_error_rel_2, profile=True)\n",
    "```\n",
    "The fit results are saved to `results_linear` and `results_exponential` (we'll need them later).\n",
    "\n",
    "Make sure to specify `profile=True` whenever you use a non-linear model function.\n",
    "A model function is linear if it is a linear function of each of its parameters.\n",
    "The model function does not need to be a linear function of the independent variable *x*.\n",
    "Examples: all polynomial model functions are linear, trigonometric functions are non-linear.\n",
    "\n",
    "If you were to now simply call `kafe2.plot` as you did before it would create a plot of just the exponential fit.\n",
    "This is because by default `kafe2.plot` creates a plot of the last fit.\n",
    "To specify that you want a fit of the last two fits, pass `-2` as the first argument:\n",
    "``` python\n",
    "kafe2.plot(\n",
    "    -2,\n",
    "    # parameter_names=dict(x=\"t\", a=r\"\\alpha\", b=r\"\\beta\", A_0=\"I_0\", x_0=\"t_0\"),\n",
    "    model_expression=[\"{a}{x} + {b}\", \"{A_0} e^{{ {x} / {x_0} }}\"]\n",
    ")\n",
    "```\n",
    "When you specify a *Python* function as a model function *kafe2* does not know how this function should be represented with LaTeX.\n",
    "The keyword argument `model_expression` passes this information to the plot.\n",
    "Parameter names have to be put between {}.\n",
    "To get {} for LaTex double them like {{ or }}.\n",
    "\n",
    "You can uncomment the keyword argument `parameter_names` to change the name of the parameters in the fit.\n",
    "\n",
    "Paste the code into the empty cell below and execute it by pressing 'shift+return'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison of two models with kafe2\n",
    "# -> insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be seeing three plots.\n",
    "The first plot is showing the data, the models, and the fit results for both fits.\n",
    "The other two plots show the profile likelihoods for the linear and the exponential fits.\n",
    "\n",
    "In the ideal case we only have static Gaussian uncertainties in *y* direction and a linear model function.\n",
    "The results obtained from the method of least squares ($\\chi^2$ method) are then optimal and unbiased.\n",
    "The conventional \"parabolic\" parameter errors like $a \\pm \\Delta a$ are then accurate.\n",
    "\n",
    "In the aforementioned ideal case the profile likelihoods of single parameters (the \"profiles\")\n",
    "(top left and bottom right) are parabolas while the profile likelihoods of two parameters\n",
    "(bottom left) are paraboloids.\n",
    "Because a paraboloid is three-dimensional it cannot be directly shown in two dimensions.\n",
    "Instead the intersects of the profile likelihood with horizontal planes above the cost\n",
    "function minimum (the \"contours\") are shown.\n",
    "In the ideal case the contours are shaped like ellipses.\n",
    "\n",
    "The profile likelihood of the exponential fit clearly deviates from the shape of a prabola/ellipse -\n",
    "this is to be expected since the exponential model is not a linear function of its parameter $x_0$.\n",
    "However, the profile likelihood of the fit using the linear model is also clearly distorted.\n",
    "The reasons for this are discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Distortions due to Uncertainties in *x* Direction\n",
    "***\n",
    "\n",
    "If errors in *x* direction are used, fitting a straight line also becomes a\n",
    "non-linear problem.\n",
    "This is because during the fit *x* errors are converted to *y* errors by multiplying\n",
    "them with the model function derivative by *x*.\n",
    "As a consequence the total error becomes a function of the fit parameters in a way\n",
    "that turns the originally linear regression into a non-linear problem.\n",
    "To illustrate this, we repeat the same fit as above with increased uncertainties on\n",
    "the x values:\n",
    "``` python\n",
    "kafe2.xy_fit(\"line\", x_data_2, y_data_2, x_error=4*x_error_2, y_error_rel=y_error_rel_2)\n",
    "kafe2.plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enter the above code here:\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be seeing that the profile likelihood has become more asymmetrical by increasing the *x* errors.\n",
    "This is because higher values for $a$ result in a higher model function derivative and thus higher total errors which reduces the cost function value.\n",
    "Conversely lower values for $a$ result in a lower model funtion derivative, lower total errors, and higher cost function values.\n",
    "While the parameter $b$ is not directly affected by *x* errors it is indirectly being affected due to its negative correlation with $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Distortions due to Relative Uncertainties in *y* Direction\n",
    "***\n",
    "\n",
    "Relative *y* errors by default also distort the profile likelihood.\n",
    "Ideally these *y* errors would be calculated to the true *y* values (which are unknown).\n",
    "As a stand-in they are calculated as relative to the model.\n",
    "This causes the *y* errors to become parameter-dependent, which in turn makes the regression problem non-linear.\n",
    "\n",
    "Alternatively relative *y* errors can be calculated as relative to the data (by setting `errors_rel_to_model=False`).\n",
    "This keeps the regression linear but it biases the fit results.\n",
    "This is because data points which randomly fluctuate to smaller values are given a smaller uncertainty than\n",
    "data points which randomly flucutate to larger values.\n",
    "\n",
    "The following code can be used to get a comparison between the two modes:\n",
    "``` python\n",
    "kafe2.xy_fit(\"line\", x_data_2, y_data_2, x_error=x_error_2, \n",
    "             y_error_rel=4*y_error_rel_2, errors_rel_to_model=True)\n",
    "kafe2.xy_fit(\"line\", x_data_2, y_data_2, x_error=x_error_2, \n",
    "             y_error_rel=4*y_error_rel_2, errors_rel_to_model=False)\n",
    "kafe2.plot(-2)\n",
    "```\n",
    "Note: The first case is the default so `errors_rel_to_model=True` can actually be omitted without changing anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the above code here:\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Output of Fit Results as Variables\n",
    "***\n",
    "\n",
    "In many applications it is necessary to continue to use the output of a fit in program code.\n",
    "This can be done by saving the return value `kafe2.xy_fit` which is a *Python* dictionary with the fit results.\n",
    "For example, the fit results of the fits with the original *x* errors were saved to `results_linear` and `results_exponential`.\n",
    "\n",
    "A formatted output (for the linear fit results) can be obtained with the following line:\n",
    "``` python\n",
    "print(\"\\n\\n\".join(f\"====== {k} ======\\n{v}\" for k, v in results_linear.items()))\n",
    "```\n",
    "When using *kafe2* objects directly these values can also be obtained by calling the objects' properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test output here\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 Hypothesis Test to Assess the Models\n",
    "***\n",
    "\n",
    "The graphical output does not clearly indicate which of the models is acceptable.\n",
    "For this purpose a hypothesis test can be performed which indicates the so-called $\\chi^2$\n",
    "probability - i.e. the probability of obtaining a worse value for $\\chi^2$ at the\n",
    "minimum than the observed one.\n",
    "A higher value corresponds to a better fit.\n",
    "\n",
    "It is calculated from the cumulative density function of the $\\chi^2$ distribution:\n",
    "``` python\n",
    "from scipy import stats\n",
    "\n",
    "def chi2prob(chi2, ndf):\n",
    "  \"\"\" chi2-probability\n",
    "\n",
    "    Args:\n",
    "      * chi2: chi2 value\n",
    "      * ndf: number of degrees of freedom\n",
    "\n",
    "    Returns:\n",
    "      * float: chi2 probability\n",
    "  \"\"\"\n",
    "\n",
    "  return 1.- stats.chi2.cdf(chi2, ndf)\n",
    "```\n",
    "\n",
    "Enter the code for the $\\chi^2$ probability in the cell and check the\n",
    "two results you got above.\n",
    "\n",
    "**Hint**: you can either copy the values for $\\chi^2$ and the number of degrees of freedom\n",
    "from the output of the previous cells or you can access them from the fit results\n",
    "saved to `results_linear` and `results_exponential`\n",
    "The fit results are dicts and you need to access the properties `goodness_of_fit` and `ndf`.\n",
    "The property `cost_function_value` is not suitable because it is the sum of $\\chi^2$ and\n",
    "correctional terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the quality of the fits\n",
    "# -> enter code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using *kafe2* outside this notebook the $\\chi^2$ probability can be accessed directly from the property `chi2_probability` of the fit results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Defining Models via SymPy\n",
    "\n",
    "If *SymPy* (Symbolic Python) is installed it can be used to define model functions.\n",
    "For example, the linear and exponential models above can be defined like this:\n",
    "\n",
    "``` python\n",
    "linear_model = \"linear_model: x a b -> a * x + b\"\n",
    "exponential_model = \"exponential_model: x A_0 x_0=5.0 -> A_0 * exp(x / x_0)\"\n",
    "```\n",
    "The string `->` marks the separation between argument definition and the model function expression.\n",
    "The beginning of the model definition up to `:` defines the model function name (can be omitted).\n",
    "\n",
    "The advantage of using the *SymPy* definitions above is that *kafe2* can use them to automatically derive LaTeX representations for the model functions.\n",
    "Repeat the fit of the two models using the two model definitions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with SymPy-defined models\n",
    "# -> enter code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Influencing the Graphical Output\n",
    "***\n",
    "\n",
    "The graphical output was suboptimal in some regards.\n",
    "For example, two data sets were shown in the legend although both models used the exact same data.\n",
    "In addition, marker properties and colors can be customized.\n",
    "However, this is a little more complicated than the e.g. changing the labels.\n",
    "The `kafe2.plot` function does not have arguments to specify things like colors.\n",
    "Instead the returned `kafe2.Plot` object (notice the capitalization) has to be used:\n",
    "``` python\n",
    "p = kafe2.Plot(-2)\n",
    "```\n",
    "\n",
    "To influence the graphic *kafe2* provides a method `Plot.customize` which can be used\n",
    "to specify values for *matplotlib* parameters for different graphic elements\n",
    "(*plot_types*: 'data', 'model_line', 'model_error_band', 'ratio', 'ratio_error_band').\n",
    "\n",
    "The parameters relevant for a *plot_type* and their current values\n",
    "can be displayed using a function of the *Plot* class:\n",
    "``` python\n",
    "p.get_keywords('model_error_band')\n",
    "```\n",
    "\n",
    "The names used for objects and possible values correspond to the names in the configuration file\n",
    "*matplotlibrc* for *matplotlib*.\n",
    "\n",
    "To for example change the name for the data set and suppress the second output, use the following call:\n",
    "``` python\n",
    "p.customize('data', 'label', [\"test data\", None])\n",
    "```\n",
    "The first argument specifies the subplot for which to set keywords.\n",
    "The second argument specifies which keyword to set.\n",
    "The third argument is a list of values to set for the keyword for each fit\n",
    "managed by the plot object.\n",
    "\n",
    "Alternatively the third argument can be a list of tuples consisting of fit indices\n",
    "and values.\n",
    "``` python\n",
    "p.customize('data', 'label', [(0, \"test data\"), (1, None)])\n",
    "```\n",
    "This syntax makes it possible to set keywords for only a part of the fits managed by the\n",
    "plot object.\n",
    "In this particular case the keyword argument `data_label` of `kafe2.plot` could also have been used.\n",
    "\n",
    "Marker type, size and color of the marker and error bars can also be customized:\n",
    "``` python\n",
    "# data\n",
    "p.customize('data', 'marker', ['o', 'o'])\n",
    "p.customize('data', 'markersize', [5, 5])\n",
    "p.customize('data', 'color', [(0, 'blue'), (1,'blue')]) # note: although 2nd label is suppressed\n",
    "p.customize('data', 'ecolor', [(0, 'blue'), (1, 'blue')]) # note: although 2nd label is suppressed\n",
    "```\n",
    "\n",
    "The corresponding values for the model function can also be customized:\n",
    "``` python\n",
    "# model\n",
    "p.customize('model_line', 'color', ['orange', 'lightgreen'])\n",
    "p.customize('model_error_band', 'label', [(0, r'$\\pm 1 \\sigma$'), (1, r'$\\pm 1 \\sigma$')])\n",
    "p.customize('model_error_band', 'color', [(0, 'orange')])\n",
    "p.customize('model_error_band', 'color', [(1, 'lightgreen')])\n",
    "```\n",
    "\n",
    "It is also possible to change parameters using *matplotlib* functions.\n",
    "To change the size of the axis labels, use the following calls:\n",
    "``` python\n",
    "# Größe der Achsenbeschriftungen\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=20, titlesize=25)\n",
    "```\n",
    "Note: the above call sets the *matplotlib* parameters globally.\n",
    "Plots unrelated to *kafe2* are also being influenced.\n",
    "\n",
    "With the *kafe2* plot object the matplotlib plot has to be manually created and shown:\n",
    "``` python\n",
    "p.plot()\n",
    "p.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Enter code to test here:\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Better Parameterization for the Exponential Model\n",
    "\n",
    "The fit results for the exponential model can be improved by a change in parameterization: $f(x; A_0, \\lambda) = A_0 e^{\\lambda x}$.\n",
    "Fit the model with the modified parameterization to the data and plot the results together with linear model and the original exponential model.\n",
    "\n",
    "**Hint**: in *Python* the name `lambda` is reserved for defining so-called anonymous functions.\n",
    "You therefore have to use a different name in your model function definition.\n",
    "The name `varlambda` is automatically displayed as the greek letter lambda in *kafe2* but in principle you can use whatever name you want as long as you set `parameter_names` when plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be seeing that the fit results are essentially unchanged by the change in parameterization.\n",
    "However, the profile of $\\lambda$ should be nearly parabolical unlike the profile of $x_0$.\n",
    "As a consequence the conventional description of value and standard deviation $\\lambda \\pm \\Delta \\lambda$ is much more accurate than the equivalent $x_0 \\pm \\Delta x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interlude: Object-Oriented Programming\n",
    "---\n",
    "Until now fits were performed using the functions `kafe2.xy_fit` and `kafe2.plot`. These functions provide\n",
    "pre-configured pipelines for fitting models to *xy* data and plotting the results. Internally they use\n",
    "objects (in the programming sense) which represent things like data or fits as a whole. From now on\n",
    "the examples will use the objects directly. This is slightly more complicated but it also provides\n",
    "greater flexibility. The general procedure is as follows:\n",
    "\n",
    "1. A data container object is created which contains data and the associated uncertainties.\n",
    "2. A fit object is created from a data container object (or raw data) and a model function.\n",
    "   As with `kafe2.xy_fit` the default model function is a straight line.\n",
    "3. The method `do_fit` of the fit object is called to perform the numerical minimization of the cost function.\n",
    "4. The fit results are extracted from the fit object. This can be done by accessing the properties of the fit\n",
    "   object, by printing a report, or by creating a plot.\n",
    "\n",
    "The following implementation is roughly equivalent to the very first example of a line fit.\n",
    "First an `XYContainer` object is created from the data.\n",
    "Errors are added to the container by calling the method `XYContainer.add_error`.\n",
    "``` python\n",
    "xy_data = XYContainer(x_data, y_data)\n",
    "xy_data.add_error('x', x_error)\n",
    "xy_data.add_error('y', y_error)\n",
    "xy_data.label = 'Data'  # How the data is called in plots\n",
    "```\n",
    "Next the fit object is created from the data container.\n",
    "In this case the method `do_fit` is called immediately but we could do other things beforehand like limiting\n",
    "or constraining parameters or adding more errors.\n",
    "``` python\n",
    "line_fit = Fit(data=xy_data)\n",
    "line_fit.do_fit()  # This will throw a warning if no errors were specified.\n",
    "```\n",
    "Finally, extract the fit results. The `Plot` class is the same as from the previous example.\n",
    "``` python\n",
    "line_fit.report()  # Prints fit results to console.\n",
    "\n",
    "plot = Plot(fit_objects=line_fit)  # Create a kafe2 plot object.\n",
    "plot.x_label = 'x'  # Set x axis label.\n",
    "plot.y_label = 'y'  # Set y axis label.\n",
    "plot.plot()  # Do the plot.\n",
    "plot.show()  # Just a convenience wrapper for matplotlib.pyplot.show() .\n",
    "```\n",
    "Note that the fit results are **not** automatically saved to disk.\n",
    "To save the fit results in this example, add `plot.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafe2 import XYContainer, Fit, Plot\n",
    "# Enter the above code here:\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Special Features of Complex (Non-Linear) Models\n",
    "---\n",
    "\n",
    "Another example of a non-linear fit is a damped oscillation of a thread pendulum.\n",
    "The corresponding measurement data are contained in the following code cell:\n",
    "``` python\n",
    "# the data\n",
    "t = [ ... ]\n",
    "t_errors = 0.05\n",
    "\n",
    "a = [ ... ]\n",
    "a_errors = 0.05\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kafe2 import XYContainer, Fit, Plot\n",
    "\n",
    "# the data:\n",
    "t = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0,\n",
    "     10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0, 14.5, 15.0, 15.5, 16.0, 16.5, 17.0, 17.5, 18.0, 18.5, 19.0,\n",
    "     19.5, 20.0, 20.5, 21.0, 21.5,22.0, 22.5, 23.0, 23.5, 24.0, 24.5, 25.0, 25.5, 26.0, 26.5, 27.0, 27.5, 28.0,\n",
    "     28.5, 29.0, 29.5, 30.0, 30.5, 31.0, 31.5, 32.0, 32.5, 33.0, 33.5, 34.0, 34.5, 35.0, 35.5, 36.0, 36.5, 37.0,\n",
    "     37.5, 38.0, 38.5, 39.0, 39.5, 40.0, 40.5, 41.0, 41.5, 42.0, 42.5, 43.0, 43.5, 44.0, 44.5, 45.0, 45.5, 46.0,\n",
    "     46.5, 47.0, 47.5, 48.0, 48.5, 49.0, 49.5, 50.0, 50.5, 51.0, 51.5, 52.0,52.5, 53.0, 53.5, 54.0, 54.5, 55.0,\n",
    "     55.5, 56.0, 56.5, 57.0, 57.5, 58.0, 58.5, 59.0, 59.5, 60.0]\n",
    "t_errors = 0.05\n",
    "\n",
    "a = [ 6.06,  5.17,  3.29,  0.64, -2.26, -4.56, -5.74, -5.58, -4.12, -1.62,\n",
    "      1.11,  3.56,  5.12,  5.43,  4.41,  2.53, -0.18, -2.78, -4.65, -5.5,\n",
    "     -5.04, -3.25, -0.75,  1.79,  3.88,  5.31,  5.2,   3.92,  1.74, -0.85,\n",
    "     -3.13, -4.71, -5.06, -4.26, -2.48, -0.13,  2.19,  4.07,  4.9,   4.64,\n",
    "      3.16,  1.17, -1.54, -3.26, -4.59, -4.64, -3.69, -1.83,  0.38,  2.76,\n",
    "      4.16,  4.58,  4.13,  2.45,  0.28, -1.8,  -3.53, -4.43, -4.31, -3.03,\n",
    "     -1.05,  1.06,  2.79,  3.97,  4.4,   3.37,  1.92, -0.14, -2.29, -3.7,\n",
    "     -4.28, -3.84, -2.44, -0.59,  1.27,  3.11,  3.9,   4.02,  2.85,  1.21,\n",
    "     -0.64, -2.51, -3.41, -3.84, -3.34, -1.75, -0.17,  1.85,  3.23,  3.72,\n",
    "      3.4,   2.54,  0.67, -1.13, -2.8,  -3.77, -3.65, -2.89, -1.43,  0.42,\n",
    "      2.2,   3.26,  3.42,  3.25,  1.88,  0.33, -1.35, -3.02, -3.41, -3.32,\n",
    "     -2.2,  -0.77,  0.92,  2.44,  3.31,  3.44,  2.77,  1.25, -0.13, -1.69, -2.78 ]\n",
    "a_errors = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amplitude as a function of time is given by the following model function:\n",
    "``` python\n",
    "# Model function for a pendulum as a one-dimensional,\n",
    "#     damped harmonic oscillator with zero initial speed:\n",
    "# s = time, a0 = initial_amplitude, l = length of the string,\n",
    "# r = radius of the steel ball, g = gravitational acceleration, c = damping coefficient.\n",
    "def damped_harmonic_oscillator(s, a0, l, r, g, c):\n",
    "  # Effective length of the pendulum = length of the string + radius of the steel ball:\n",
    "  l_total = l + r\n",
    "  omega_0 = np.sqrt(g / l_total) # Phase speed of an undamped pendulum.\n",
    "  omega_d = np.sqrt(omega_0 ** 2 - c ** 2) # Phase speed of a damped pendulum.\n",
    "  return a0 * np.exp(-c * s) * (np.cos(omega_d * s) + c / omega_d * np.sin(omega_d * s))\n",
    "```\n",
    "\n",
    "Data container and fit object are created as usual:\n",
    "``` python\n",
    "# Create data container:\n",
    "data3 = XYContainer(t, a)\n",
    "data3.add_error(axis='x', err_val=t_errors)\n",
    "data3.add_error(axis='y', err_val=a_errors)\n",
    "data3.axis_labels = ('Time t (s)', 'Amplitude A (°)') \n",
    "\n",
    "# Create fit object from data and model function:\n",
    "fit = Fit(data3, damped_harmonic_oscillator)\n",
    "```\n",
    "\n",
    "The model contains a number of parameters defined by \"auxiliary measurements\".\n",
    "``` python\n",
    "# Relevant physical magnitudes and their uncertainties:\n",
    "lm, delta_lm = 10.000, 0.002  # length of the string, l = 10.0 +- 0.002 m\n",
    "rm, delta_rm = 0.052, 0.001  # radius of the steel ball, r = 0.052 +- 0.001 m\n",
    "# Amplitude of the steel ball at x=0 in degrees, a0m = 6 +- 1% degrees:\n",
    "a0m, delta_a0m = 6.0, 0.01  # Note that the uncertainty on a0m is relative to a0m.\n",
    "```\n",
    "\n",
    "The fit takes this into account by considering the corresponding parameters both as\n",
    "parameters of the fit and as additional data points.\n",
    "In *kafe2* such parameters restricted by measurements are considered with the help of the\n",
    "method *Fit.add_parameter_constraint()* and their uncertainties are propagated into\n",
    "the result of the fit:\n",
    "``` python\n",
    "# Constrain model parameters to measurements:\n",
    "fit.add_parameter_constraint(name='l', value=lm, uncertainty=delta_lm)\n",
    "fit.add_parameter_constraint(name='r', value=rm, uncertainty=delta_rm)\n",
    "fit.add_parameter_constraint(name='a0', value=a0m, uncertainty=delta_a0m, relative=True)\n",
    "```\n",
    "\n",
    "Alternatively, you could set the parameters to constant values with the method\n",
    "*Fit.fix_parameter()*;\n",
    "however, the uncertainties on the final result of the fit would then have to be calculated\n",
    "using classical error propagation.\n",
    "\n",
    "A problem with  non-linear fits is that there are often secondary minima of the cost\n",
    "function - convergence to the global minimum is not guaranteed.\n",
    "It is therefore necessary to select \"reasonable\" start parameters for the fit.\n",
    "This is done using the function *Fit.set_parameter_values()*:\n",
    "``` python\n",
    "g_initial = 9.81  # Initial guess for g.\n",
    "fit.set_parameter_values(g=g_initial, a0=a0m, l=lm, r=rm)\n",
    "```\n",
    "\n",
    "If the initial values are completely unknown, the fit should be repeated with a wide range\n",
    "of initial parameters to check that it consistently converges to the same minimum.\n",
    "\n",
    "Another means of improving convergence is to limit parameters to \"reasonable\" intervals.\n",
    "The parameters *a0*, *l*, and *r* are for example positive by definition.\n",
    "During the fit however they can become negative.\n",
    "The following code limits them to positive values:\n",
    "``` python\n",
    "fit.limit_parameter(\"a0\", lower=1e-6)\n",
    "fit.limit_parameter(\"l\", lower=1e-6)\n",
    "fit.limit_parameter(\"r\", lower=1e-6)\n",
    "```\n",
    "For technical reasons parameters can only be limited to closed intervals.\n",
    "As the lower limit a small value close to zero is specified.\n",
    "Because no upper value is specified the parameter limit is one-sided.\n",
    "It is also possible to define two-sided parameter limits:\n",
    "``` python\n",
    "fit.limit_parameter(\"g\", lower=9.71, upper=9.91)\n",
    "```\n",
    "The above limit is based on the assessment that results outside of these limits are\n",
    "very unlikely.\n",
    "It's also a good idea to limit parameters based on the physical properties of the system.\n",
    "For example the model function only yields real solutions for $c < \\frac{g}{l + r}$.\n",
    "This can be considered with the following code:\n",
    "``` python\n",
    "c_max = 0.9 * g_initial / (lm + rm)  # A little lower than our best guess for the limit.\n",
    "fit.limit_parameter(\"c\", lower=1e-6, upper=c_max)\n",
    "```\n",
    "\n",
    "After these preparations the fit can be performed as usual.\n",
    "The following code example also shows how to access the fit results via properties if\n",
    "they are to be processed further in the program or if a specific output is desired.\n",
    "``` python\n",
    "# Perform the fit\n",
    "fit.do_fit()\n",
    "# Optional: Print out a report on the fit results on the console.\n",
    "#fit.report(show_data=False, show_model=False, show_fit_results=True)\n",
    "\n",
    "# Custom printout of results:\n",
    "print(\"cost function at minimum: %.4g \" % fit.cost_function_value,\n",
    "    \" number of degrees of freedom:\", fit.ndf)\n",
    "print(\" --> probability: %.1f%%\" % (fit.chi2_probability * 100))\n",
    "print(\"parameter names:\\n\", fit.parameter_names)\n",
    "np.set_printoptions(precision=5, suppress=False)\n",
    "print(\"prameter values:\\n\", fit.parameter_values)\n",
    "print(\"parameter uncertainties:\\n\",fit.parameter_errors)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"correlation matrix:\\n\", fit.parameter_cor_mat )\n",
    "      \n",
    "# Optional: plot the fit results.\n",
    "plot = Plot(fit)\n",
    "plot.plot(fit_info=True)\n",
    "plot.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Enter code here\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Construction of a Covariance Matrix from Individual Uncertainties\n",
    "---\n",
    "\n",
    "To be treated:\n",
    "  - dealing with complex uncertainties\n",
    "\n",
    "One of the strengths of _kafe2_ is the support for correlated uncertainties.\n",
    "This refers to contributions to uncertainties that influence some or all values in\n",
    "the same way - e.g. because they were recorded with the same measuring instrument\n",
    "that has a systematic uncertainty.\n",
    "Uncertainties that are shared between groups of measured values are very common.\n",
    "\n",
    "The following function is used to specify uncertainties:\n",
    "> `add_error**( [axis], err_val, name=None, correlation=0, relative=False)`  \n",
    "  Add an uncertainty source to the data container. Returns an error id which\n",
    "  uniquely identifies the created error source.  \n",
    "  **Parameters**  \n",
    "  • axis (str or int) – 'x'/0 or 'y'/1  \n",
    "  • err_val (float or iterable of float) – pointwise uncertainty/uncertainties for all data points  \n",
    "  • name (str or None) – unique name for this uncertainty source. If None, the name\n",
    "    of the error source will be set to a random alphanumeric string.  \n",
    "  • correlation (float) – correlation coefficient between any two distinct data points  \n",
    "  • relative (bool) – if True, err_val will be interpreted as a relative uncertainty  \n",
    "  **Returns** error name  \n",
    "  **Return type** str  \n",
    "\n",
    "It belongs to the container class, but can also be called via a fit class.\n",
    "With this rather simple interface, independent uncertainties as well as\n",
    "common absolute or relative uncertainties of data points can be specified.\n",
    "The specified uncertainties are converted into a covariance matrix of the data points.\n",
    "If the interface is called several times, the resulting covariance matrices are added\n",
    "together (in accordance with the rules of elementary error propagation).\n",
    "\n",
    "A very simple example will illustrate this.\n",
    "We consider the averaging of four values, which were carried out by two groups with different\n",
    "measuring methods.\n",
    "Each of the two groups gives two measurements;\n",
    "in the first group there is an absolute uncertainty common to the two measurements;\n",
    "the second group indicates a relative uncertainty correlated between its two measurements, e.g.\n",
    "caused by a scaling error.\n",
    "Furthermore, the measurements are based on a common (theoretical) assumption which leads to an\n",
    "absolute uncertainty common to all measurements.\n",
    "\n",
    "For this simple problem we use the simplest data structure of *kafe2*, the\n",
    "_IndexedContainer_, to provide the data:\n",
    "``` python\n",
    "from kafe2 import IndexedContainer\n",
    "idx_data = IndexedContainer([5.3, 5.2, 4.7, 4.8])\n",
    "```\n",
    "As model we choose a constant function:\n",
    "``` python\n",
    "# The very simple \"model\":\n",
    "def average (a):\n",
    "  return a\n",
    "```\n",
    "\n",
    "The uncertainties are then stated as follows\n",
    "                (Note: For an _IndexedContainer_, the _axis_ parameter is not required!):\n",
    "  1. each measurement has its own independent uncertainty\n",
    "   `err_stat = idx_data.add_error([.2, .2, .2, .2])`\n",
    "  2. the uncertainty common to the first two values\n",
    "   `err_syst12 = idx_data.add_error([.175, .175, 0., 0.], correlation = 1.)`\n",
    "  3. the relative uncertainty common to the last two values\n",
    "   `err_syst34 = idx_data.add_error([0., 0., .05, 0.05], correlation = 1., relative=True)`\n",
    "  4. the uncertainty common to all values\n",
    "   `err_syst = idx_data.add_error(0.15, correlation = 1.)`\n",
    "\n",
    "We should also provide suitable labels for the data:\n",
    "``` python\n",
    "idx_data.label = 'Test data'\n",
    "idx_data.axis_labels = [None,'Measured value (a.o.)']\n",
    "```\n",
    "\n",
    "The execution of the fit is well known by now:\n",
    "``` python\n",
    "# Set up the fit:\n",
    "ifit = Fit(idx_data, average)\n",
    "ifit.model_label = 'average value'\n",
    "\n",
    "# Perform the fit:\n",
    "ifit.do_fit()\n",
    "```\n",
    "\n",
    "The results can of course be obtained with the _report()_ function, if necessary also as a\n",
    "graphical representation:\n",
    "``` python\n",
    "# Report and plot results:\n",
    "ifit.report()\n",
    "p=Plot(ifit)\n",
    "p.plot()\n",
    "p.show()\n",
    "```\n",
    "\n",
    "If a problem contains several contributions to the overall uncertainty, one would\n",
    "usually like to study the influence of individual components.\n",
    "For this purpose, one can comfortably work with the functions *disable_error()* and\n",
    "*enable_error()* and make appropriate fits:\n",
    "``` python\n",
    "print(\"disabling common sysytematic error\")\n",
    "idx_data.disable_error(err_syst)\n",
    "_ifit = Fit(idx_data, average)\n",
    "_ifit.do_fit()\n",
    "_ifit.report()\n",
    "#      do not forget to switch on again\n",
    "idx_data.enable_error(err_syst)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Enter code here\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Application from the Real World: Fit of a Breit-Wigner Resonance\n",
    "---\n",
    "\n",
    "To be treated:\n",
    "  - dealing with complex uncertainties\n",
    "  - the creation of an appealing graphical output\n",
    "  - studying the influence of individual error components\n",
    "\n",
    "Typically, the uncertainties of the measurement data are much more complex than in the examples\n",
    "discussed so far.\n",
    "In most cases there are uncertainties in ordinate and abscissa, and in addition to the\n",
    "independent uncertainties of each data point there are common, correlated uncertainties for all\n",
    "of them.\n",
    "\n",
    "With the method *add_error()* or *add_matrix_error()* uncertainties can be specified on the 'x'\n",
    "and 'y' data, either in the form of independent or correlated, relative or absolute uncertainties\n",
    "of all or groups of measured values or by specifying the complete covariance or correlation matrix.\n",
    "All uncertainties specified in this way are included in the global covariance matrix for the fit.\n",
    "\n",
    "As an example, we consider measurements of a cross section as a function of the energy near a\n",
    "resonance.\n",
    "These are combined measurement data from the four experiments at CERN's LEP accelerator, which\n",
    "were corrected for effects caused by photon radiation:\n",
    "\n",
    "Measurements of the hadronic cross section $\\sigma_{e^+e^- \\to {\\rm hadrons}}$ as a function of\n",
    "the centre-of-mass energy $E$.\n",
    "``` python\n",
    "## Data:\n",
    "# Center-of-mass energy E (GeV)\n",
    "E = [ 88.387, 89.437, 90.223, 91.238, 92.059, 93.004, 93.916 ]\n",
    "E_errors = [ 0.005, 0.0015, 0.005, 0.003, 0.005, 0.0015, 0.005 ]\n",
    "ECor_abs = 0.0017  # correlated absolute errors\n",
    "\n",
    "# hadronic cross section with photonic corrections applied (nb)\n",
    "sig = [6.803, 13.965, 26.113, 41.364, 27.535, 13.362, 7.302 ]\n",
    "sig_errors = [ 0.036, 0.013, 0.075, 0.010, 0.088, 0.015, 0.045 ]\n",
    "sigCor_rel = 0.0007\n",
    "```\n",
    "\n",
    "As a model we use a modified Breit-Wigner resonance with a width dependent on the centre-of-mass\n",
    "energy (\"$s$-dependent width\", where $s = E_{CM}^2$):\n",
    "``` python\n",
    "## Model:\n",
    "# Breit-Wigner with s-dependent width\n",
    "def BreitWigner(E, s0 = 41.0, M = 91.2, G = 2.5):\n",
    "    s = E*E\n",
    "    Msq = M*M\n",
    "    Gsq = G*G\n",
    "    return s0*s*Gsq/((s-Msq)*(s-Msq)+(s*s*Gsq/Msq))\n",
    "```\n",
    "\n",
    "The data container with the uncertainties is created as follows:\n",
    "``` python\n",
    "BWdata= XYContainer(ECM, sig)\n",
    "# Add independent errors:\n",
    "error_name_sig = BWdata.add_error(axis='x', name = 'deltaE', err_val = E_errors )\n",
    "error_name_E = BWdata.add_error(axis='y', name = 'deltaSig', err_val = sig_errors )\n",
    "# Add fully correlated, absolute Energy errors:\n",
    "error_name_ECor = BWdata.add_error(axis='x', name='Ecor',err_val = ECor_abs, correlation = 1.)\n",
    "# Add fully correlated, relative cross section errors:\n",
    "error_name_sigCor = BWdata.add_error(axis='y', name='sigCor',\n",
    "                            err_val = sigCor_rel, correlation = 1., relative=True)\n",
    "```\n",
    "\n",
    "Whether the uncertainties are independent or correlated is determined by the parameter\n",
    "*correlation*;\n",
    "for independent uncertainties it is zero, for uncertainties common to all data entries it is one.\n",
    "Values between 0. and 1. are also allowed;\n",
    "however, in practice the covariance matrix for describing the overall uncertainty is usually\n",
    "composed of uncorrelated and fully correlated components.\n",
    "The names given in the *add_error* function allow the individual error components to be accessed\n",
    "later.\n",
    "\n",
    "Fit and result output follow the usual procedure:\n",
    "``` python\n",
    "BWfit = Fit(BWdata, BreitWigner)\n",
    "BWfit.do_fit()\n",
    "BWfit.report(asymmetric_parameter_errors=True)\n",
    "# Optional: plot the fit results\n",
    "BWplot = Plot(BWfit)\n",
    "BWplot.plot(asymmetric_parameter_errors=True)\n",
    "BWplot.show()\n",
    "```\n",
    "\n",
    "**Enhancement of the graphical output**\n",
    "To ensure that the type of data is clearly described, suitable names should be assigned.\n",
    "The lines below must be inserted before the *Fit* object is created.\n",
    "``` python\n",
    "BWdata.label = 'QED-corrected hadronic cross-sections'\n",
    "BWdata.axis_labels = ('CM Energy (GeV)', '$\\sigma_h$ (nb)' )\n",
    "```\n",
    "Alternatively the following lines can be inserted after creating the fit object:\n",
    "``` python\n",
    "BWfit.data_container.label = 'QED-corrected hadronic cross-sections'\n",
    "BWfit.data_container.axis_labels = ('CM Energy (GeV)', r'$\\sigma_h$ (nb)')\n",
    "```\n",
    "\n",
    "A suitable name for the model should also be set in the legend for the graphical output.\n",
    "To do this, insert the line below after the *Fit* object has been created:\n",
    "``` python\n",
    "BWfit.model_label = 'Beit-Wigner with s-dependent width'\n",
    "```\n",
    "\n",
    "If a nicely set expression for the model function is desired, LaTeX names can be set for the\n",
    "model, the parameters and the model function:\n",
    "``` python\n",
    "# Set LaTeX names for printout in info-bo:\n",
    "BWfit.assign_parameter_latex_names(E='E', s0=r'{\\sigma^0}', M=r'{M_Z}', G=r'{\\Gamma_Z}')\n",
    "BWfit.assign_model_function_latex_name(r'\\sigma^{\\rm ew}_{e^+e^-\\to{\\rm hadrons}}')\n",
    "BWfit.assign_model_function_latex_expression(\n",
    "               r'{s0}\\frac{{ {E}^2{G}^2}}{{({E}^2-{M}^2)^2+({E}^4{G}^2/{M}^2)}}')\n",
    "```\n",
    "Note: The doubling of the brackets \"{\" and \"}\" is necessary because in *kafe2*, similar to the\n",
    "Python *format* function, they are also used to pass parameters.\n",
    "\n",
    "The previous example showed how to modify the band showing the model uncertainty:\n",
    "``` python\n",
    "BWplot.customize('model_error_band', 'label', [r'$\\pm 1\\sigma$'])\n",
    "```\n",
    "In this example, however, the model uncertainty is extremely small (well below 0.1%) and\n",
    "therefore not visible in the graph.\n",
    "You can suppress the output in the legend with the following specification:\n",
    "``` python\n",
    "BWplot.customize('model_error_band', 'label', [None])\n",
    "```\n",
    "Sometimes the uncertainty band is covered by the line;\n",
    "in such cases a dashed or dotted line should be used for the model:\n",
    "``` python\n",
    "BWplot.customize('model_line', 'linestyle', [':'])\n",
    "```\n",
    "\n",
    "The edges of the plot can also be adjusted.\n",
    "This can be done via the properties *x_range* and *y_range* od the Plot class:\n",
    "``` python\n",
    "BWplot.x_range = (88, 94)\n",
    "BWplot.y_range = (0, 45)\n",
    "```\n",
    "\n",
    "Since this is a non-linear fit, the profile likelihood and confidence contours should\n",
    "still be displayed.\n",
    "The following line must be inserted before *BWplot.show()*:\n",
    "``` python\n",
    "ContoursProfiler(BWfit).plot_profiles_contours_matrix(show_grid_for='contours')\n",
    "```\n",
    "!!! Patience: the calculation of the contours is computationally complex and takes a certain\n",
    "amount of time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kafe2 import ContoursProfiler  # kafe2 object for contour plots\n",
    "\n",
    "''' the data for the Breit-Wigner example'''\n",
    "# Center-of-mass energies E (GeV)\n",
    "ECM = [ 88.387, 89.437, 90.223, 91.238, 92.059, 93.004, 93.916 ]\n",
    "E_errors = [ 0.005, 0.0015, 0.005, 0.003, 0.005, 0.0015, 0.005 ]\n",
    "ECor_abs = 0.0017  # correlated absolute errors\n",
    "\n",
    "# hadronic cross sections with photonic corrections applied (nb)\n",
    "sig = [6.803, 13.965, 26.113, 41.364, 27.535, 13.362, 7.302 ]\n",
    "sig_errors = [ 0.036, 0.013, 0.075, 0.010, 0.088, 0.015, 0.045 ]\n",
    "sigCor_rel = 0.0007\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# enter the code from above here\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Study of the influence of individual error components**\n",
    "To investigate the influence of individual error components on the result, individual sources of\n",
    "uncertainty can be switched off with the method *disable_error()* and a new fit can be performed,\n",
    "shown here for the correlated uncertainty of the center-of-mass energies:\n",
    "``` python\n",
    "print('!!!  disabling error component ', error_name_ECor)\n",
    "BWfit.disable_error(error_name_ECor)\n",
    "BWfit.do_fit()\n",
    "BWfit.report(show_data=False, show_model=False)\n",
    "\n",
    "# do not forget to switch on again !\n",
    "print('!!!  re-enabling error component ', error_name_ECor)\n",
    "BWfit.enable_error(error_name_ECor)\n",
    "\n",
    "#### fallback option with new fit object\n",
    "#print('!!!  disabling error component ', error_name_ECor)\n",
    "#BWdata.disable_error(error_name_ECor)\n",
    "#_fit = Fit(BWdata, BreitWigner)\n",
    "#_fit.do_fit()\n",
    "#_fit.report(show_data=False, show_model=False)\n",
    "#BWdata.enable_error(error_name_ECor)\n",
    "```\n",
    "The result is almost identical to the previous one, only the uncertainty of the mass is now smaller.\n",
    "This was also to be expected, because a correlated change of all energies should not influence\n",
    "the width or height of the resonance.\n",
    "\n",
    "With the method `enable_error(error_name_ECor)` the uncertainty source is reactivated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try it out here\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6. Fit to Histogram Data\n",
    "***\n",
    "\n",
    "In principle, the fit of a distribution density to a frequency distribution can also be\n",
    "understood as a functional fit.\n",
    "However, there are some special features that must be taken into account:\n",
    "\n",
    "- The function value for a bin corresponding to the value of a distribution density (PDF=Particle\n",
    "  Density Function) corresponds to the integral of the PDF via the bin\n",
    "- The uncertainty of a bin entry results from the Poisson distribution, which can only be\n",
    "  approximated by a Gaussian distribution for very large numbers of entries per bin.\n",
    "\n",
    "Therefore *kafe2* offers a special method to fit a distribution density to histograms, the\n",
    "classes *HistContainer* to store the histogram data and *HistFit* to perform the fit:\n",
    "``` python\n",
    "from kafe2 import HistContainer, HistFit\n",
    "```\n",
    "\n",
    "Twice the negative logarithm of the Poisson likelihood is used as the cost function to evaluate\n",
    "the agreement of the adjusted PDF with the bin entries in the frequency distribution, other\n",
    "options are configurable.\n",
    "\n",
    "In this simple example, we use the frequency distribution of Gaussian-distributed random numbers\n",
    "to which a Gaussian distribution is fitted.\n",
    "``` python\n",
    "def normal_distribution_pdf(x, mu, sigma):\n",
    "  return np.exp(-0.5 * ((x - mu) / sigma) ** 2) / np.sqrt(2.0 * np.pi * sigma** 2)\n",
    "```\n",
    "\n",
    "The data is generated randomly from the standard normal distribution:\n",
    "``` python\n",
    "# create a random dataset of 100 random values,\n",
    "#  following a standard normal distribution with mu=0 and sigma=1\n",
    "data = np.random.normal(loc=0, scale=1, size=100)\n",
    "```\n",
    "\n",
    "The data container and the fit object are created in the same way as in the previous examples:\n",
    "``` python\n",
    "# Create a histogram from the dataset by specifying the bin range and the number of bins.\n",
    "# Alternatively the bin edges can be set.\n",
    "histogram = HistContainer(n_bins=10, bin_range=(-5, 5), fill_data=data)\n",
    "\n",
    "# create the Fit object by specifying a density function\n",
    "fit = HistFit(data=histogram, model_function=normal_distribution_pdf)\n",
    "```\n",
    "\n",
    "Carrying out the fit and outputting the results are no different from the procedure used in the\n",
    "previous examples:\n",
    "``` python\n",
    "# do the fit\n",
    "fit.do_fit()\n",
    "\n",
    "# Optional: print a report to the terminal\n",
    "fit.report()\n",
    "\n",
    "# Optional: create a plot and show it\n",
    "phist = Plot(fit)\n",
    "phist.plot()\n",
    "phist.show()\n",
    "```\n",
    "\n",
    "At this point we should take another look at the possibilities for customizing the graphical output.\n",
    "The plot adapter for histograms knows the values *data*, *model* and *model_density* as *plot_type*.\n",
    "By calling `print(phist.get_keywords(<plot_type>))` the adjustable parameters can be retrieved.\n",
    "Here is a suggestion for code to customize the graphic output, which must be placed before the\n",
    "*phist.plot()* command:\n",
    "``` python\n",
    "## reprise: plot customization\n",
    "#    data\n",
    "phist.customize('data', 'label', [\"random Gaussian data\"] )\n",
    "phist.customize('data', 'marker', ['o'])\n",
    "phist.customize('data', 'markersize', [5])\n",
    "phist.customize('data', 'color', ['blue'])\n",
    "phist.customize('data', 'ecolor', ['blue'])\n",
    "#    model\n",
    "phist.customize('model_density', 'label', [\"Gaussian PDF\"])\n",
    "phist.customize('model_density', 'color', [\"black\"])\n",
    "phist.customize('model', 'label', [\"entries per bin\"])\n",
    "phist.customize('model', 'facecolor', [\"lightgrey\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try here\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7. Likelihood Fits\n",
    "***\n",
    "\n",
    "If only few measurements are available it is not possible to obtain a meaningful frequency\n",
    "distribution because a coarse division into bins would distort the measurements, while a too\n",
    "fine division would lead to bins with very few or even zero entries.\n",
    "The method already used above for fitting a distribution density to a frequency distribution is\n",
    "then not applicable.\n",
    "In such cases, a direct fit to the unbinned data using the maximum likelihood method is used.\n",
    "This procedure is implemented in *kafe2*.\n",
    "For this the appropriate classes must be imported:\n",
    "``` python\n",
    "from kafe2.fit import UnbinnedContainer, UnbinnedFit\n",
    "```\n",
    "\n",
    "In this example we use 160 individual measurements of the lifetime of muons from cosmic\n",
    "radiation stopped in a detector.\n",
    "The frequency distribution is an exponential distribution over flat ground:\n",
    "\n",
    "``` python\n",
    "def pdf(t, tau=2.2, fbg=0.1, a=1., b=9.75):\n",
    "  \"\"\"\n",
    "  Probability density function for the decay time of a myon.\n",
    "  The pdf is normalized to an integral of one for the interval (a, b).\n",
    "  :param t: decay time\n",
    "  :param fbg: background\n",
    "  :param tau: expected mean of the decay time\n",
    "  :param a: the minimum decay time which can be measured\n",
    "  :param b: the maximum decay time which can be measured\n",
    "  :return: probability for decay time x\n",
    "  \"\"\"\n",
    "  pdf1 = np.exp(-t / tau) / tau / (np.exp(-a / tau) - np.exp(-b / tau))\n",
    "  pdf2 = 1. / (b - a)\n",
    "  return (1 - fbg) * pdf1 + fbg * pdf2\n",
    "```\n",
    "\n",
    "Please note that the frequency distribution for all possible parameter values must be normalized\n",
    "to one!\n",
    "\n",
    "There is only one small particularity regarding the fit procedure:\n",
    "due to the small number of observations, the background portion is subject to a large uncertainty\n",
    "and can therefore even become negative during the fit.\n",
    "To avoid this \"unphysical\" range of the parameter, the previously shown method\n",
    "`fit.limit_parameter(<name>, lower=<min>, upper=<max>)` can be used.\n",
    "\n",
    "All further steps in the following sample code are already known:\n",
    "``` python\n",
    "data = UnbinnedContainer(dT) # create the kafe data object\n",
    "data.label = 'lifetime measurements'\n",
    "data.axis_labels = ('Myon Life Time ' r'$\\tau$' ' (µs)','Density' )\n",
    "\n",
    "# create the fit object and set the pdf for the fit\n",
    "LLfit = UnbinnedFit(data=data, model_function = pdf)\n",
    "\n",
    "# assign latex names for model and parameters for nicer display\n",
    "LLfit.model_label = 'Exponential decay + flat background'\n",
    "LLfit.assign_parameter_latex_names(t='t', tau=r'\\tau', fbg='f', a='a', b='b')\n",
    "LLfit.assign_model_function_latex_expression(\"\\\\frac{{ (1-{fbg}) \\, e^{{-{0}/{tau}}}}}\"\n",
    "    \"{{{tau}(e^{{-{a}/{tau}}}-e^{{-{b}/{tau}}})}} + \\\\frac{{ {fbg} }} {{ {b}-{a} }}\")\n",
    "\n",
    "# Fix the parameters a and b ...\n",
    "a = 1.0\n",
    "b = 11.5\n",
    "LLfit.fix_parameter(\"a\", a)\n",
    "LLfit.fix_parameter(\"b\", b)\n",
    "# ... and limit parameter fbg\n",
    "LLfit.limit_parameter(\"fbg\", lower=0., upper=1.)\n",
    "\n",
    "LLfit.do_fit()  # perform the fit\n",
    "LLfit.report(asymmetric_parameter_errors=True)\n",
    "\n",
    "pLL = Plot(LLfit)  # create a plot object\n",
    "pLL.x_range = [a, b]\n",
    "pLL.plot(fit_info=True, asymmetric_parameter_errors=True)  # plot the data and the fit\n",
    "#pLL.axes[0]['main'].set_xlabel('Life time '+r'$\\tau$'+' (µs)', size='large')  # overwrite the x-axis label\n",
    "\n",
    "cpfLL = ContoursProfiler(LLfit, profile_subtract_min=False)  # Optional: create a contours profile\n",
    "cpfLL.plot_profiles_contours_matrix(parameters=['tau', 'fbg'])  # Optional: plot the contour matrix for tau and fbg\n",
    "\n",
    "cpfLL.show()  # show the plot(s)\n",
    "```\n",
    "\n",
    "Of particular interest is the special mode of graphical representation of the data,\n",
    "where each measured value is represented by a line.\n",
    "The density of the lines per unit length corresponds to the distribution density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "''' the data for the myon life time example'''\n",
    "# real data from measurement with a Water Cherenkov detector (\"Kamiokanne\")\n",
    "dT = [7.42, 3.773, 5.968, 4.924,  1.468,  4.664,  1.745,  2.144,  3.836,  3.132,\n",
    "  1.568,  2.352,  2.132,  9.381,  1.484,  1.181,  5.004,  3.06,   4.582,  2.076,\n",
    "  1.88,   1.337,  3.092,  2.265,  1.208,  2.753,  4.457,  3.499,  8.192,  5.101,\n",
    "  1.572,  5.152,  4.181,  3.52,   1.344, 10.29,   1.152,  2.348,  2.228,  2.172,\n",
    "  7.448,  1.108,  4.344,  2.042,  5.088,  1.02,   1.051,  1.987,  1.935,  3.773,\n",
    "  4.092,  1.628,  1.688,  4.502,  4.687,  6.755,  2.56,   1.208,  2.649,  1.012,\n",
    "  1.73,   2.164,  1.728,  4.646,  2.916,  1.101,  2.54,   1.02,   1.176,  4.716,\n",
    "  9.671,  1.692,  9.292, 10.72,   2.164,  2.084,  2.616,  1.584,  5.236,  3.663,\n",
    "  3.624,  1.051,  1.544,  1.496,  1.883,  1.92,   5.968,  5.89,   2.896,  2.76,\n",
    "  1.475,  2.644,  3.6,    5.324,  8.361,  3.052,  7.703,  3.83,   1.444,  1.343,\n",
    "  4.736,  8.7,    6.192,  5.796,  1.4,    3.392,  7.808,  6.344,  1.884,  2.332,\n",
    "  1.76,   4.344,  2.988,  7.44,   5.804,  9.5,    9.904,  3.196,  3.012,  6.056,\n",
    "  6.328,  9.064,  3.068,  9.352,  1.936,  1.08,   1.984,  1.792,  9.384, 10.15,\n",
    "  4.756,  1.52,   3.912,  1.712, 10.57,   5.304,  2.968,  9.632,  7.116, 1.212,\n",
    "  8.532,  3.000,  4.792,  2.512,  1.352,  2.168,  4.344,  1.316,  1.468, 1.152,\n",
    "  6.024,  3.272,  4.96,  10.16,   2.14,   2.856, 10.01,   1.232, 2.668, 9.176 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try the likelihood fit here\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 8. Multi-Fits:\n",
    "### simultaneous fit of model functions to different data sets\n",
    "***\n",
    "Very often the models are too complex to determine all parameters in a fit to a single model.\n",
    "Instead model parameters are frequently the result of several fits,\n",
    "or the same parameter occurs in different measurement series.\n",
    "\n",
    "For such cases *kafe2* offers the possibility to perform several fits of different models with\n",
    "common parameters to different data sets.\n",
    "\n",
    "For this purpose, the *MultiFit* package must also be imported:\n",
    "``` python\n",
    "from kafe2 import MultiFit\n",
    "```\n",
    "\n",
    "As a simple example, we consider the determination of an ohmic resistance at room temperature,\n",
    "which heats up at higher current flow and thus changes its resistance according to its\n",
    "temperature coefficient.\n",
    "Therefore, in addition to the current through the resistor, the temperature is measured for each\n",
    "given voltage value.\n",
    "Triplets of measured values must therefore be evaluated.\n",
    "\n",
    "The temperature dependence is described empirically by a simple quadratic model:\n",
    "``` python\n",
    "# empirical model for T(U): a parabola\n",
    "def empirical_T_U_model(U, p2=1.0, p1=1.0, p0=0.0):\n",
    "    # use quadratic model as empirical temperature dependence T(U)\n",
    "    return p2 * U**2 + p1 * U + p0\n",
    "```\n",
    "\n",
    "The resistance as a function of temperature is given by the temperature coefficient $\\alpha$ and\n",
    "is modeled as follows\n",
    "``` python\n",
    "# model of current-voltage dependence I(U) for a heating resistor\n",
    "def I_U_model(U, R0=1., alph=0.004, p2=1.0, p1=1.0, p0=0.0):\n",
    "    # use quadratic model as empirical temperature dependence T(U)\n",
    "    t_ref = 0.\n",
    "    _delta_t = empirical_T_U_model(U, p2, p1, p0) - t_ref\n",
    "    # plug the temperature into the model\n",
    "    return U / (R0 * (1.0 + _delta_t * alph))\n",
    "```\n",
    "\n",
    "So in this case the model for resistance contains the first model for the dependence of\n",
    "temperature on the current determined by the applied voltage.\n",
    "\n",
    "Here is the data for this example:\n",
    "``` python\n",
    "# the data\n",
    "U = [ 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5,\n",
    "      6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0 ]\n",
    "I = [ 0.5,  0.89, 1.41, 1.67, 2.3,  2.59, 2.77, 3.57, 3.94,  4.24, 4.73,\n",
    "      4.87, 5.35, 5.74, 5.77, 6.17, 6.32, 6.83, 6.87, 7.17 ]\n",
    "T = [ 20.35, 20.65, 22.25, 23.65, 26.25, 27.85, 29.85, 34.25, 37.75, 41.95,\n",
    "     44.85, 50.05, 54.25, 60.55, 65.05, 69.95, 76.85, 81.55, 85.45, 94.75 ]\n",
    "sigU, sigI, sigT = 0.2, 0.1, 0.5 # uncertainties\n",
    "```\n",
    "\n",
    "The Fit procedure is hardly different from the procedure presented so far.\n",
    "First, the data containers and fits for the two models are defined as:\n",
    "``` python\n",
    "# Step 1: construct the singular data containers and fit objects\n",
    "TU_data = XYContainer(U,T)\n",
    "TU_data.label = 'Temperature data'\n",
    "TU_data.axis_labels = ['Voltage (V)','Temperature (°C)']\n",
    "fit_1 = Fit(TU_data, model_function=empirical_T_U_model)\n",
    "fit_1.model_label = 'Parametrization'\n",
    "\n",
    "IU_data = XYContainer(U,I)\n",
    "IU_data.label = 'Current data'\n",
    "IU_data.axis_labels = ['Voltage (V)','Current (A)']\n",
    "fit_2 = Fit(IU_data, model_function=I_U_model)\n",
    "fit_2.model_label = 'Temperature-dependent conductance'\n",
    "\n",
    "```\n",
    "\n",
    "Then both fits are combined to a *MultiFit*.\n",
    "``` python\n",
    "# Step 2: construct a MultiFit object\n",
    "multi_fit = MultiFit(fit_list=[fit_1, fit_2], minimizer='iminuit')\n",
    "```\n",
    "Only now are the uncertainties added - this time to the fit objects.\n",
    "This also allows the uncertainties on the x-axis, which are common to both data sets, to be taken\n",
    "into account.\n",
    "``` python\n",
    "# Step 3: Add errors (to the fit object in this case)\n",
    "multi_fit.add_error(sigT, 0, axis='y')  # declare errors on T\n",
    "multi_fit.add_error(sigI, 1, axis='y')  # declare errors on I\n",
    "multi_fit.add_error(sigU, 'all', axis='x') # shared error on x axis\n",
    "```\n",
    "\n",
    "The next step is to define meaningful names for the output:\n",
    "``` python\n",
    "# (Optional): assign names for models and parameters\n",
    "multi_fit.assign_parameter_latex_names(\n",
    "    U='U', p2='p_2', p1='p_1', p0='p_0', R0='R_0', alph=r'\\alpha_\\mathrm{T}')\n",
    "multi_fit.assign_model_function_expression('{p2}*{U}^2 + {p1}*{U} + {p0}', fit_index=0)\n",
    "multi_fit.assign_model_function_latex_expression(r'{p2}\\,{U}^2 + {p1}\\,{U} + {p0}', fit_index=0)\n",
    "multi_fit.assign_model_function_expression('{U} / ({R0} * (1 + ({p2}*{U}^2 + {p1}*{U} + {p0}) * {alph}))', fit_index=1)\n",
    "multi_fit.assign_model_function_latex_expression(r'\\frac{{{U}}}{{{R0} \\cdot (1 + ({p2}{U}^2 + {p1}{U} + {p0}) \\cdot {alph})}}', fit_index=1)\n",
    "```\n",
    "\n",
    "The rest works the same way as before:\n",
    "``` python\n",
    "# Step 4: do the fit\n",
    "multi_fit.do_fit()\n",
    "\n",
    "# (Optional): print the results\n",
    "multi_fit.report()\n",
    "\n",
    "# (Optional): plot the results\n",
    "plot = Plot(multi_fit, separate_figures=True)\n",
    "plot.customize('data', 'marker', ['.','.'])\n",
    "plot.customize('data', 'markersize', [6,6])\n",
    "\n",
    "plot.plot()\n",
    "\n",
    "plot.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# enter own code here\n",
    "# -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
